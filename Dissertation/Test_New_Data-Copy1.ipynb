{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351ea723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "###imports###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from scipy import io\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "#import skimage.io\n",
    "import random\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "from keras.models import Model\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f9d145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "NUM_CLASSES = 102\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e731521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c7ff6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.keras.preprocessing.image.ImageDataGenerator"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.preprocessing.image.ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a48cd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dataset_dir = 'E:/Github/thesis/flowerDataset/Flower/'\n",
    "labels_dir = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "new_flower_dataset_dir = 'E:/Github/thesis/flowerDataset/new_flower_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e13df28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = io.loadmat(labels_dir)\n",
    "data_labels = data_labels.items()\n",
    "data_labels = list(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0d6498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ff0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[1][3][0] #loads the labels that is stored in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3547d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(orig_dataset_dir):\n",
    "    images = []\n",
    "    for i, filename in enumerate(os.listdir(orig_dataset_dir)):\n",
    "        #check label[i] value\n",
    "        label = str(labels[i])        \n",
    "        dst_dir = new_flower_dataset_dir+label\n",
    "        if os.path.isdir(dst_dir): #check if director exist the copy file\n",
    "            shutil.copy(orig_dataset_dir+filename, dst_dir)\n",
    "        else: #create folder with name label and copy over file \n",
    "            os.makedirs(dst_dir)\n",
    "            shutil.copy(orig_dataset_dir+filename, dst_dir)\n",
    "            \n",
    "        #if label value folder exists? if not create it else copy image[filename] to new directory\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5293a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_images_from_folder(orig_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05c4711e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8189 files belonging to 102 classes.\n",
      "Using 7371 files for training.\n",
      "Found 8189 files belonging to 102 classes.\n",
      "Using 1637 files for validation.\n"
     ]
    }
   ],
   "source": [
    "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    new_flower_dataset_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",  # categorical, binary\n",
    "    # class_names=['0', '1', '2', '3', ...]\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),  # reshape if not in this size\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.1,\n",
    "    subset=\"training\",\n",
    ")\n",
    "\n",
    "ds_validation = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    new_flower_dataset_dir,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",  # categorical, binary\n",
    "    # class_names=['0', '1', '2', '3', ...]\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch_size,\n",
    "    image_size=(img_height, img_width),  # reshape if not in this size\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c9df4fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: image*'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c569cdca6a93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mds_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"image*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[1;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[0;32m   1112\u001b[0m           string_ops.reduce_join(file_pattern, separator=\", \"), name=\"message\")\n\u001b[0;32m   1113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0m\u001b[0;32m   1115\u001b[0m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[0;32m   1116\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[1;34m\"\"\"Decorates the input function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[0;32m    248\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m                                      error_in_function=error_in_function)\n",
      "\u001b[1;32m~\\.conda\\envs\\tf2\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[1;34m(condition, data, summarize, name)\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_n_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[0mdata_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_summarize_eager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m       raise errors.InvalidArgumentError(\n\u001b[0m\u001b[0;32m    155\u001b[0m           \u001b[0mnode_def\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m           \u001b[0mop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: image*'"
     ]
    }
   ],
   "source": [
    "ds_train.list_files(\"image*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "883307c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def augment(x, y):\\n    image = tf.image.random_brightness(x, max_delta=0.05)\\n    return image, y\\n\\n\\nds_train = ds_train.map(augment)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def augment(x, y):\n",
    "    image = tf.image.random_brightness(x, max_delta=0.05)\n",
    "    return image, y\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(augment)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "effbc32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.learndatasci.com/tutorials/hands-on-transfer-learning-keras/\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0201c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "    \"\"\"\n",
    "    Compiles a model integrated with VGG16 pretrained layers\n",
    "    \n",
    "    input_shape: tuple - the shape of input images (width, height, channels)\n",
    "    n_classes: int - number of classes for the output layer\n",
    "    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n",
    "    fine_tune: int - The number of pre-trained layers to unfreeze.\n",
    "                If set to 0, all pretrained layers will freeze during training\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pretrained convolutional layers are loaded using the Imagenet weights.\n",
    "    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n",
    "    conv_base = VGG16(include_top=False,\n",
    "                     weights='imagenet', \n",
    "                     input_shape=input_shape)\n",
    "    \n",
    "    # Defines how many layers to freeze during training.\n",
    "    # Layers in the convolutional base are switched from trainable to non-trainable\n",
    "    # depending on the size of the fine-tuning parameter.\n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Create a new 'top' of the model (i.e. fully-connected layers).\n",
    "    # This is 'bootstrapping' a new top_model onto the pretrained layers.\n",
    "    top_model = conv_base.output\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(4096, activation='relu')(top_model)\n",
    "    top_model = Dense(1072, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.2)(top_model)\n",
    "    output_layer = Dense(NUM_CLASSES, activation='softmax')(top_model)\n",
    "    \n",
    "    # Group the convolutional base and new fully-connected layers into a Model object.\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "\n",
    "    # Compiles the model for training.\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b29d9fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "optim_1 = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "#n_steps = ds_train.samples // BATCH_SIZE\n",
    "#n_val_steps = ds_validation.samples // BATCH_SIZE\n",
    "\n",
    "# First we'll train the model without Fine-tuning\n",
    "vgg_model = create_model(input_shape, NUM_CLASSES, optim_1, fine_tune=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4343e06b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "231/231 - 18s - loss: 4.6118 - accuracy: 0.0301 - val_loss: 4.6008 - val_accuracy: 0.0367\n",
      "Epoch 2/100\n",
      "231/231 - 18s - loss: 4.5875 - accuracy: 0.0309 - val_loss: 4.5818 - val_accuracy: 0.0367\n",
      "Epoch 3/100\n",
      "231/231 - 17s - loss: 4.5681 - accuracy: 0.0309 - val_loss: 4.5670 - val_accuracy: 0.0367\n",
      "Epoch 4/100\n",
      "231/231 - 17s - loss: 4.5525 - accuracy: 0.0309 - val_loss: 4.5554 - val_accuracy: 0.0367\n",
      "Epoch 5/100\n",
      "231/231 - 17s - loss: 4.5401 - accuracy: 0.0309 - val_loss: 4.5465 - val_accuracy: 0.0367\n",
      "Epoch 6/100\n",
      "231/231 - 17s - loss: 4.5304 - accuracy: 0.0309 - val_loss: 4.5396 - val_accuracy: 0.0367\n",
      "Epoch 7/100\n",
      "231/231 - 17s - loss: 4.5227 - accuracy: 0.0309 - val_loss: 4.5345 - val_accuracy: 0.0367\n",
      "Epoch 8/100\n",
      "231/231 - 16s - loss: 4.5169 - accuracy: 0.0309 - val_loss: 4.5307 - val_accuracy: 0.0367\n",
      "Epoch 9/100\n",
      "231/231 - 17s - loss: 4.5124 - accuracy: 0.0309 - val_loss: 4.5279 - val_accuracy: 0.0367\n",
      "Epoch 10/100\n",
      "231/231 - 17s - loss: 4.5090 - accuracy: 0.0301 - val_loss: 4.5260 - val_accuracy: 0.0367\n",
      "Epoch 11/100\n",
      "231/231 - 18s - loss: 4.5064 - accuracy: 0.0288 - val_loss: 4.5247 - val_accuracy: 0.0367\n",
      "Epoch 12/100\n",
      "231/231 - 18s - loss: 4.5045 - accuracy: 0.0279 - val_loss: 4.5237 - val_accuracy: 0.0367\n",
      "Epoch 13/100\n",
      "231/231 - 17s - loss: 4.5030 - accuracy: 0.0281 - val_loss: 4.5232 - val_accuracy: 0.0367\n",
      "Epoch 14/100\n",
      "231/231 - 18s - loss: 4.5019 - accuracy: 0.0305 - val_loss: 4.5229 - val_accuracy: 0.0244\n",
      "Epoch 15/100\n",
      "231/231 - 17s - loss: 4.5010 - accuracy: 0.0313 - val_loss: 4.5226 - val_accuracy: 0.0244\n",
      "Epoch 16/100\n",
      "231/231 - 17s - loss: 4.5004 - accuracy: 0.0313 - val_loss: 4.5226 - val_accuracy: 0.0244\n",
      "Epoch 17/100\n",
      "231/231 - 17s - loss: 4.4999 - accuracy: 0.0313 - val_loss: 4.5227 - val_accuracy: 0.0244\n",
      "Epoch 18/100\n",
      "231/231 - 17s - loss: 4.4996 - accuracy: 0.0313 - val_loss: 4.5227 - val_accuracy: 0.0244\n",
      "Epoch 19/100\n",
      "231/231 - 18s - loss: 4.4993 - accuracy: 0.0313 - val_loss: 4.5227 - val_accuracy: 0.0244\n",
      "Epoch 20/100\n",
      "231/231 - 18s - loss: 4.4991 - accuracy: 0.0313 - val_loss: 4.5229 - val_accuracy: 0.0244\n",
      "Epoch 21/100\n",
      "231/231 - 17s - loss: 4.4989 - accuracy: 0.0313 - val_loss: 4.5229 - val_accuracy: 0.0244\n",
      "Epoch 22/100\n",
      "231/231 - 17s - loss: 4.4988 - accuracy: 0.0313 - val_loss: 4.5231 - val_accuracy: 0.0244\n",
      "Epoch 23/100\n",
      "231/231 - 17s - loss: 4.4987 - accuracy: 0.0313 - val_loss: 4.5231 - val_accuracy: 0.0244\n",
      "Epoch 24/100\n",
      "231/231 - 18s - loss: 4.4986 - accuracy: 0.0313 - val_loss: 4.5231 - val_accuracy: 0.0244\n",
      "Epoch 25/100\n",
      "231/231 - 18s - loss: 4.4986 - accuracy: 0.0313 - val_loss: 4.5233 - val_accuracy: 0.0244\n",
      "Epoch 26/100\n",
      "231/231 - 18s - loss: 4.4986 - accuracy: 0.0313 - val_loss: 4.5233 - val_accuracy: 0.0244\n",
      "Epoch 27/100\n",
      "231/231 - 17s - loss: 4.4985 - accuracy: 0.0313 - val_loss: 4.5234 - val_accuracy: 0.0244\n",
      "Epoch 28/100\n",
      "231/231 - 18s - loss: 4.4985 - accuracy: 0.0313 - val_loss: 4.5235 - val_accuracy: 0.0244\n",
      "Epoch 29/100\n",
      "231/231 - 17s - loss: 4.4985 - accuracy: 0.0313 - val_loss: 4.5235 - val_accuracy: 0.0244\n",
      "Epoch 30/100\n",
      "231/231 - 18s - loss: 4.4985 - accuracy: 0.0313 - val_loss: 4.5236 - val_accuracy: 0.0244\n",
      "Epoch 31/100\n",
      "231/231 - 18s - loss: 4.4985 - accuracy: 0.0313 - val_loss: 4.5237 - val_accuracy: 0.0244\n",
      "Epoch 32/100\n",
      "231/231 - 18s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5236 - val_accuracy: 0.0244\n",
      "Epoch 33/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5236 - val_accuracy: 0.0244\n",
      "Epoch 34/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5237 - val_accuracy: 0.0244\n",
      "Epoch 35/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5237 - val_accuracy: 0.0244\n",
      "Epoch 36/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5237 - val_accuracy: 0.0244\n",
      "Epoch 37/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5237 - val_accuracy: 0.0244\n",
      "Epoch 38/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5237 - val_accuracy: 0.0244\n",
      "Epoch 39/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5237 - val_accuracy: 0.0244\n",
      "Epoch 40/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5238 - val_accuracy: 0.0244\n",
      "Epoch 41/100\n",
      "231/231 - 18s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5238 - val_accuracy: 0.0244\n",
      "Epoch 42/100\n",
      "231/231 - 18s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5239 - val_accuracy: 0.0244\n",
      "Epoch 43/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5239 - val_accuracy: 0.0244\n",
      "Epoch 44/100\n",
      "231/231 - 17s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5240 - val_accuracy: 0.0244\n",
      "Epoch 45/100\n",
      "231/231 - 19s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5240 - val_accuracy: 0.0244\n",
      "Epoch 46/100\n",
      "231/231 - 18s - loss: 4.4984 - accuracy: 0.0313 - val_loss: 4.5239 - val_accuracy: 0.0244\n",
      "Epoch 47/100\n"
     ]
    }
   ],
   "source": [
    "vgg_history = vgg_model.fit(ds_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=ds_validation,\n",
    "                            verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307261b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'elephant.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c803e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64b41ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0573a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5281150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3172ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8beea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df37cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158e187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
