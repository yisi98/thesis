{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737f5f14",
   "metadata": {},
   "source": [
    "# Dissertation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3073d9",
   "metadata": {},
   "source": [
    "## Research into the techniques and methods to achieve state of the art accuracy in flower species identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97bbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6984ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###imports###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from scipy import io\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "#import skimage.io\n",
    "import random\n",
    "import PIL\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa11d15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a43f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbda12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///E:/Github/thesis/Dissertation/mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed53ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecf1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes all outputs be in float format rather than exponentials\n",
    "#np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c860a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "IMG_SIZE = 224 #this parameter sets image dimensions as 50*50\n",
    "DATE = datetime.datetime.now().strftime('%d-%b-%Y')\n",
    "MODEL_PATH = f'models/{DATE}/'\n",
    "MODEL_NAME = 'FlowerClassifierTrial.model'.format(int(time.time()))\n",
    "log_dir=f'logs\\\\{MODEL_NAME}'\n",
    "TENSORBOARD = TensorBoard(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c687c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ed7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbdd12",
   "metadata": {},
   "source": [
    "# Load Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1c4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Directory for MacBook\n",
    "# mac_URL = '/Volumes/T7/Uni/Github/thesis/flowerDataset/Flower'\n",
    "# mac_labels = '/Volumes/T7/Uni/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "# mac_dataSplit = '/Volumes/T7/Uni/Github/thesis/flowerDataset/setid.mat'\n",
    "# mac_loaded_images = '/Volumes/T7/Uni/loaded_images.npy'\n",
    "# mac_image_train='/Volumes/T7/Uni/image_train.npy'\n",
    "# mac_label_train = '/Volumes/T7/Uni/label_train.npy'\n",
    "# mac_image_test = '/Volumes/T7/Uni/image_test.npy'\n",
    "# mac_label_test = '/Volumes/T7/Uni/label_test.npy'\n",
    "# mac_image_val = '/Volumes/T7/Uni/image_val.npy'\n",
    "# mac_label_val = '/Volumes/T7/Uni/label_val.npy'\n",
    "\n",
    "# ###Directory for PC\n",
    "# pc_URL = 'E:/Github/thesis/flowerDataset/Flower'\n",
    "# pc_labels = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "# pc_dataSplit = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "# pc_loaded_images = 'E:/Dissertation/data/loaded_images.npy'\n",
    "# pc_image_train = 'E:/Dissertation/data/image_train.npy'\n",
    "# pc_label_train = 'E:/Dissertation/data/label_train.npy'\n",
    "# pc_image_test = 'E:/Dissertation/data/image_test.npy'\n",
    "# pc_label_test = 'E:/Dissertation/data/label_test.npy'\n",
    "# pc_image_val = 'E:/Dissertation/data/image_val.npy'\n",
    "# pc_label_val = 'E:/Dissertation/data/label_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1180309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories():\n",
    "    if os.path.exists(mac_URL) and os.path.exists(mac_labels) and os.path.exists(\n",
    "            mac_dataSplit) and os.path.exists(mac_loaded_images):\n",
    "        return mac_URL, mac_labels, mac_dataSplit, mac_loaded_images, mac_image_train, mac_label_train, mac_image_test, mac_label_test,mac_image_val, mac_label_val \n",
    "    else:\n",
    "        return pc_URL, pc_labels, pc_dataSplit, pc_loaded_images, pc_image_train, pc_label_train, pc_image_test, pc_label_test, pc_image_val, pc_label_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ec1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL_dir, labels_dir, dataSplit_dir, loaded_images_dir, image_train_dir, label_train_dir, image_test_dir, label_test_dir, image_val_dir, label_val_dir = get_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af5a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_dir = 'E:/Github/thesis/flowerDataset/Flower'\n",
    "labels_dir = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "dataSplit_dir = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "loaded_images_dir = 'E:/Dissertation/data/loaded_images.npy'\n",
    "image_train_dir ='E:/Dissertation/data/image_train.npy'\n",
    "label_train_dir = 'E:/Dissertation/data/label_train.npy'\n",
    "image_test_dir = 'E:/Dissertation/data/image_test.npy'\n",
    "label_test_dir = 'E:/Dissertation/data/label_test.npy'\n",
    "image_val_dir = 'E:/Dissertation/data/image_val.npy'\n",
    "label_val_dir = 'E:/Dissertation/data/label_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591cf84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Github/thesis/flowerDataset/imagelabels.mat\n"
     ]
    }
   ],
   "source": [
    "print(labels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0c1a",
   "metadata": {},
   "source": [
    "### Load DataSet and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17b53f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = io.loadmat(labels_dir)\n",
    "data_labels = data_labels.items()\n",
    "data_labels = list(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c2554af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41c40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[1][3][0] #loads the labels that is stored in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a0b6511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 77, 77, ..., 62, 62, 62], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed0a2854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b102eefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_LABELS = np.unique(labels)\n",
    "UNIQUE_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ff41223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got loading files code from link below\n",
    "#https://stackoverflow.com/questions/30230592/loading-all-images-using-imread-from-a-given-folder\n",
    "#Function gets all the images loaded up\n",
    "def load_images_from_folder(URL):\n",
    "    images = []\n",
    "    for filename in os.listdir(URL):\n",
    "        #img = cv2.imread(os.path.join(URL,filename))\n",
    "        img = cv2.resize(cv2.imread(os.path.join(URL,filename), cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "        img = np.reshape(img,[IMG_SIZE,IMG_SIZE,3])\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    np.save(loaded_images_dir, images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "563d6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to run this cell if image already loaded.\n",
    "#load_images_from_folder(URL_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b32b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if images are already loaded, just load the file here rather than running function again.\n",
    "image_data = np.load(loaded_images_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0969fb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8189, 224, 224, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows the shape of the image_data, the number of images, the dimensions and number of colour channels\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5eec5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4356b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine image with labels\n",
    "def randomised_dataset(image_data, labels):\n",
    "    np.random.seed(8)\n",
    "    indices = np.arange(image_data.shape[0])\n",
    "    print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    image_data = image_data[indices]\n",
    "    labels = labels[indices]\n",
    "    return image_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "965b929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_data, labels = randomised_dataset(image_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cebed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8189, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4efd03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Label Encoder\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "converted_labels = np.array(mlb.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09ba97fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b6fb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(labels):\n",
    "    new_labels = []\n",
    "    for i in range(len(labels)):\n",
    "        new_labels.insert(i, [labels[i]])\n",
    "    return np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7430f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted_labels = convert_label(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e40f31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52f70b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf733688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6dc2e",
   "metadata": {},
   "source": [
    "# Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64e30768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citation code from https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "#splitting the image dataset into the ratio for training, validation and testing data\n",
    "def split_data(image_data, converted_labels):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\n",
    "    image_train, image_test, label_train, label_test = train_test_split(image_data, converted_labels, test_size=1 - train_ratio, random_state=42)\n",
    "\n",
    "    #the test from previous line which is 25% of dataset is passed into the line below to be \n",
    "    #further split into 15% for validation and 10% for testing\n",
    "\n",
    "    image_val, image_test, label_val, label_test = train_test_split(image_test, label_test, test_size=test_ratio/(test_ratio + validation_ratio),random_state=42) \n",
    "\n",
    "    print('image_train',image_train.shape)\n",
    "    print('label_train',label_train.shape)\n",
    "    print('image_test',image_test.shape)\n",
    "    print('label_test',label_test.shape)\n",
    "    print('image_val', image_val.shape)\n",
    "    print('label_val', label_val.shape)\n",
    "\n",
    "    np.save(image_train_dir, image_train)\n",
    "    np.save(label_train_dir, label_train)\n",
    "    np.save(image_test_dir, image_test)\n",
    "    np.save(label_test_dir, label_test)\n",
    "    np.save(image_val_dir, image_val)\n",
    "    np.save(label_val_dir, label_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d985813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train (6141, 224, 224, 3)\n",
      "label_train (6141, 102)\n",
      "image_test (820, 224, 224, 3)\n",
      "label_test (820, 102)\n",
      "image_val (1228, 224, 224, 3)\n",
      "label_val (1228, 102)\n"
     ]
    }
   ],
   "source": [
    "split_data(image_data,converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aee55642",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.load(image_train_dir, allow_pickle=True)\n",
    "image_test = np.load(image_test_dir, allow_pickle=True)\n",
    "label_train = np.load(label_train_dir, allow_pickle=True)\n",
    "label_test = np.load(label_test_dir, allow_pickle=True)\n",
    "image_val = np.load(image_val_dir, allow_pickle=True)\n",
    "label_val = np.load(label_val_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "606a621e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 224, 224, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "952d08fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a1187",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff6920dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the images to the range of 0 - 1.0 \n",
    "image_train = image_train/255.0\n",
    "image_test = image_test/255.0\n",
    "image_val = image_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1bb389a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 224, 224, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e534027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 102)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "312de917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 102)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90564584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8328cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55d0ae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of classes/ flowers\n",
    "NUM_CLASSES = 102\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "040ac502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 102)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1925a731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18204d6a",
   "metadata": {},
   "source": [
    "# Custom CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc375b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of model\n",
    "name_model = 'test-{}'.format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81c56ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "verbose= 2\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a6597d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(act):\n",
    "    # tf.reset_default_graph()\n",
    "    model = Sequential()\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # INPUT LAYER\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation(act))\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # HIDDEN LAYER 1\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # HIDDEN LAYER 2\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 3\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # Fully Connected\n",
    "    model.add(Flatten()) # converts the 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.25)) # reduces overfitting\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6803cb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_cnn_model(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53d5fff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 222, 222, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 220, 220, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 220, 220, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 110, 110, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 108, 108, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 54, 54, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               9437440   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 102)               26214     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 102)               0         \n",
      "=================================================================\n",
      "Total params: 9,863,238\n",
      "Trainable params: 9,862,278\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2c8b375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit(image_train, label_train, batch_size=64, epochs=100, validation_data=(image_val, label_val), verbose=1, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a7e4887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "#model.save(f'{MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff2059af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "#model =  tf.keras.models.load_model(f'{MODEL_PATH}')\n",
    "#model = tf.keras.models.load_model(f'E:/GoogleSync/Masters/Dissertation/models/24-Jun-2021') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170a1d3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27a9ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c30171a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/07/20 16:03:13 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of keras. If you encounter errors during autologging, try upgrading / downgrading keras to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "192/192 - 22s - loss: 5.9657 - accuracy: 0.0272 - val_loss: 5.3825 - val_accuracy: 0.0277\n",
      "Epoch 2/30\n",
      "192/192 - 8s - loss: 4.8507 - accuracy: 0.0430 - val_loss: 4.6972 - val_accuracy: 0.0375\n",
      "Epoch 3/30\n",
      "192/192 - 10s - loss: 4.6201 - accuracy: 0.0480 - val_loss: 4.7579 - val_accuracy: 0.0562\n",
      "Epoch 4/30\n",
      "192/192 - 10s - loss: 4.4781 - accuracy: 0.0624 - val_loss: 5.7796 - val_accuracy: 0.0529\n",
      "Epoch 5/30\n",
      "192/192 - 8s - loss: 4.3832 - accuracy: 0.0697 - val_loss: 4.7996 - val_accuracy: 0.0635\n",
      "Epoch 6/30\n",
      "192/192 - 8s - loss: 4.2895 - accuracy: 0.0842 - val_loss: 4.8722 - val_accuracy: 0.0700\n",
      "Epoch 7/30\n",
      "192/192 - 8s - loss: 4.2256 - accuracy: 0.0871 - val_loss: 4.5326 - val_accuracy: 0.0627\n",
      "Epoch 8/30\n",
      "192/192 - 8s - loss: 4.1621 - accuracy: 0.0953 - val_loss: 4.6941 - val_accuracy: 0.0741\n",
      "Epoch 9/30\n",
      "192/192 - 8s - loss: 4.0492 - accuracy: 0.1045 - val_loss: 4.7657 - val_accuracy: 0.0692\n",
      "Epoch 10/30\n",
      "192/192 - 8s - loss: 3.9755 - accuracy: 0.1135 - val_loss: 4.5018 - val_accuracy: 0.0717\n",
      "Epoch 11/30\n",
      "192/192 - 8s - loss: 3.8693 - accuracy: 0.1192 - val_loss: 4.5576 - val_accuracy: 0.0733\n",
      "Epoch 12/30\n",
      "192/192 - 8s - loss: 3.7429 - accuracy: 0.1309 - val_loss: 4.2192 - val_accuracy: 0.0945\n",
      "Epoch 13/30\n",
      "192/192 - 8s - loss: 3.6480 - accuracy: 0.1462 - val_loss: 4.1977 - val_accuracy: 0.1042\n",
      "Epoch 14/30\n",
      "192/192 - 11s - loss: 3.5692 - accuracy: 0.1650 - val_loss: 5.0408 - val_accuracy: 0.0814\n",
      "Epoch 15/30\n",
      "192/192 - 8s - loss: 3.4493 - accuracy: 0.1811 - val_loss: 4.2350 - val_accuracy: 0.1197\n",
      "Epoch 16/30\n",
      "192/192 - 8s - loss: 3.3836 - accuracy: 0.1969 - val_loss: 4.4005 - val_accuracy: 0.1189\n",
      "Epoch 17/30\n",
      "192/192 - 10s - loss: 3.2762 - accuracy: 0.2006 - val_loss: 3.8981 - val_accuracy: 0.1376\n",
      "Epoch 18/30\n",
      "192/192 - 8s - loss: 3.1407 - accuracy: 0.2299 - val_loss: 4.0335 - val_accuracy: 0.1580\n",
      "Epoch 19/30\n",
      "192/192 - 8s - loss: 3.0448 - accuracy: 0.2521 - val_loss: 3.6061 - val_accuracy: 0.1564\n",
      "Epoch 20/30\n",
      "192/192 - 8s - loss: 2.9202 - accuracy: 0.2682 - val_loss: 3.8384 - val_accuracy: 0.1450\n",
      "Epoch 21/30\n",
      "192/192 - 8s - loss: 2.7582 - accuracy: 0.3011 - val_loss: 3.7690 - val_accuracy: 0.1938\n",
      "Epoch 22/30\n",
      "192/192 - 8s - loss: 2.6289 - accuracy: 0.3280 - val_loss: 3.7670 - val_accuracy: 0.2085\n",
      "Epoch 23/30\n",
      "192/192 - 8s - loss: 2.4996 - accuracy: 0.3514 - val_loss: 3.5911 - val_accuracy: 0.2541\n",
      "Epoch 24/30\n",
      "192/192 - 9s - loss: 2.3350 - accuracy: 0.3853 - val_loss: 3.4019 - val_accuracy: 0.2606\n",
      "Epoch 25/30\n",
      "192/192 - 15s - loss: 2.2305 - accuracy: 0.4068 - val_loss: 3.4653 - val_accuracy: 0.2410\n",
      "Epoch 26/30\n",
      "192/192 - 12s - loss: 2.0393 - accuracy: 0.4465 - val_loss: 3.4309 - val_accuracy: 0.2932\n",
      "Epoch 27/30\n",
      "192/192 - 13s - loss: 1.9267 - accuracy: 0.4674 - val_loss: 3.3505 - val_accuracy: 0.3265\n",
      "Epoch 28/30\n",
      "192/192 - 11s - loss: 1.7565 - accuracy: 0.5058 - val_loss: 3.1860 - val_accuracy: 0.3046\n",
      "Epoch 29/30\n",
      "192/192 - 8s - loss: 1.6295 - accuracy: 0.5351 - val_loss: 3.4960 - val_accuracy: 0.2606\n",
      "Epoch 30/30\n",
      "192/192 - 8s - loss: 1.5070 - accuracy: 0.5706 - val_loss: 3.1858 - val_accuracy: 0.3445\n",
      "26/26 - 2s - loss: 2.8771 - accuracy: 0.3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/07/20 16:09:01 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "mlflow.keras.autolog()\n",
    "\n",
    "\n",
    "\n",
    "with mlflow.start_run():\n",
    "        \n",
    "        #model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "        #optimizer=tf.keras.optimizers.Adam(),\n",
    "        #metrics=['accuracy'])\n",
    "        # Fit our model\n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_param(\"activation function\", act)\n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"repetition\", irep)\n",
    "        mlflow.log_param(\"validation_samples\", len(y_val))\n",
    "        mlflow.log_param(\"test_samples\", len(y_test))\n",
    "        mlflow.log_param(\"training_samples\", len(y_train))\n",
    "        mlflow.log_param(\"epochs\", EPOCHS)\n",
    "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "        mlflow.log_param(\"augment\", iaug)\n",
    "        mlflow.log_param(\"nreps\", CV)\n",
    "        mlflow.log_param(\"scaling\", scaling)\n",
    "        mlflow.log_param(\"neural_ner\", ner_type)\n",
    "        mlflow.log_param(\"neural_ner_model\", neural_ner_model)\n",
    "        mlflow.log_param(\"neg_augmentation_samples\", i_neg_augmentation_samples)\n",
    "        mlflow.log_param(\"embedding_model\", EMBED_MODEL)\n",
    "        mlflow.log_param(\"ner_model\", NER_MODEL)\n",
    "        mlflow.log_param(\"input_file\", INPUT_FILE)\n",
    "        mlflow.log_param(\"delta_hrs_cutoff\", HARD_DELTA_CUTOFF_HRS)\n",
    "        \n",
    "        \n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        #mlflow.keras.log_model(model, \"standardCNN\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "209e3b03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\yisi9\\.conda\\envs\\tf3080\\Scripts\\mlflow.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\click\\core.py\", line 1137, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\click\\core.py\", line 1062, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\click\\core.py\", line 1668, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\click\\core.py\", line 1404, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\click\\core.py\", line 763, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\mlflow\\cli.py\", line 280, in ui\n",
      "    _run_server(backend_store_uri, default_artifact_root, host, port, None, 1)\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\mlflow\\server\\__init__.py\", line 138, in _run_server\n",
      "    exec_cmd(full_command, env=env_map, stream_output=True)\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\site-packages\\mlflow\\utils\\process.py\", line 34, in exec_cmd\n",
      "    child = subprocess.Popen(\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\users\\yisi9\\.conda\\envs\\tf3080\\lib\\subprocess.py\", line 1420, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "FileNotFoundError: [WinError 2] The system cannot find the file specified\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui --backend-store-uri file:///C:\\Users\\yisi9\\.conda\\envs\\tf3080\\mlflow-server\\mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e64051ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: mlflow server [OPTIONS]\n",
      "Try 'mlflow server --help' for help.\n",
      "\n",
      "Error: Got unexpected extra arguments (default-artifact-root file:///C:\\Users\\yisi9\\.conda\\envs\\tf3080\\mlflow-server\\mlruns)\n"
     ]
    }
   ],
   "source": [
    "!mlflow server --backend-store-uri file:///C:\\Users\\yisi9\\.conda\\envs\\tf3080\\mlflow-server\\mlruns -- default-artifact-root file:///C:\\Users\\yisi9\\.conda\\envs\\tf3080\\mlflow-server\\mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db851db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: mlflow server [OPTIONS]\n",
      "\n",
      "  Run the MLflow tracking server.\n",
      "\n",
      "  The server which listen on http://localhost:5000 by default, and only accept\n",
      "  connections from the local machine. To let the server accept connections\n",
      "  from other machines, you will need to pass ``--host 0.0.0.0`` to listen on\n",
      "  all network interfaces (or a specific interface address).\n",
      "\n",
      "Options:\n",
      "  --backend-store-uri PATH     URI to which to persist experiment and run\n",
      "                               data. Acceptable URIs are SQLAlchemy-compatible\n",
      "                               database connection strings (e.g.\n",
      "                               'sqlite:///path/to/file.db') or local\n",
      "                               filesystem URIs (e.g.\n",
      "                               'file:///absolute/path/to/directory'). By\n",
      "                               default, data will be logged to the ./mlruns\n",
      "                               directory.\n",
      "  --default-artifact-root URI  Local or S3 URI to store artifacts, for new\n",
      "                               experiments. Note that this flag does not\n",
      "                               impact already-created experiments. Default:\n",
      "                               Within file store, if a file:/ URI is provided.\n",
      "                               If a sql backend is used, then this option is\n",
      "                               required.\n",
      "  -h, --host HOST              The network address to listen on (default:\n",
      "                               127.0.0.1). Use 0.0.0.0 to bind to all\n",
      "                               addresses if you want to access the tracking\n",
      "                               server from other machines.\n",
      "  -p, --port INTEGER           The port to listen on (default: 5000).\n",
      "  -w, --workers TEXT           Number of gunicorn worker processes to handle\n",
      "                               requests (default: 4).\n",
      "  --static-prefix TEXT         A prefix which will be prepended to the path of\n",
      "                               all static paths.\n",
      "  --gunicorn-opts TEXT         Additional command line options forwarded to\n",
      "                               gunicorn processes.\n",
      "  --waitress-opts TEXT         Additional command line options for waitress-\n",
      "                               serve.\n",
      "  --expose-prometheus TEXT     Path to the directory where metrics will be\n",
      "                               stored. If the directory doesn't exist, it will\n",
      "                               be created. Activate prometheus exporter to\n",
      "                               expose metrics on /metrics endpoint.\n",
      "  --help                       Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!mlflow server --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6b1f5962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef9ac114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 11ms/step - loss: 2.8771 - accuracy: 0.3927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8770852088928223, 0.3926829397678375]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function that returns loss value & metrics values\n",
    "model.evaluate(image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b3ec96c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db31c612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 12888), started 2 days, 7:29:59 ago. (Use '!kill 12888' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-44fa9b68cb0bef19\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-44fa9b68cb0bef19\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "86168ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(image_test, model):\n",
    "    #going through each of the test images\n",
    "    count = 0\n",
    "    for i in range(len(image_test)): #going through all test images\n",
    "        img = image_test[i] #load in the image\n",
    "        \n",
    "        img = img.reshape(-1, 100, 100, 3) #get it in the right shape for model.predict\n",
    "        \n",
    "        prediction = model.predict(img).flatten() #using model predict to get what it thinks is the answer\n",
    "        \n",
    "        # prediction.astype(np.int) #converts predictions to integers\n",
    "        \n",
    "        pred_index = np.argmax(prediction) # grabs the INDEX of the best prediction\n",
    "        \n",
    "        max_pred = max(prediction) # grabs the SCORE of best prediction\n",
    "        # print(prediction)\n",
    "        # print(pred_index)\n",
    "        \n",
    "        test_label_value = label_test[i][pred_index] # gets the actual test label at the same index\n",
    "        # print(test_label_value)\n",
    "        if test_label_value == 1:\n",
    "            #print(\"CORRECT PREDICTION\")\n",
    "            count += 1\n",
    "            \n",
    "#         # FIND THE INDEX WHERE THE VALUE = 1\n",
    "#         for j in range(len(predicted_label)):\n",
    "#             if predicted_label[j] == 1 : #find the point when it finds 1 \n",
    "#                 if label_test[i][j] == 1: #check if at this exact point is also 1  whic means correct prediction\n",
    "#                     count+=1 #increase number of correct count by 1\n",
    "#                     print(count)\n",
    "#                     break\n",
    "    print(f\"num correct - {count}\")\n",
    "    accuracy = (count/len(label_test))*100\n",
    "    per_symbol = '%'\n",
    "    print(f'accuracy is {accuracy} {per_symbol}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "446de09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 224, 224, 3)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19ad03f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 150528 into shape (100,100,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-ddadfef6b03a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-68-e8b78b7d7964>\u001b[0m in \u001b[0;36mprediction_accuracy\u001b[1;34m(image_test, model)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#load in the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#get it in the right shape for model.predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#using model predict to get what it thinks is the answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 150528 into shape (100,100,3)"
     ]
    }
   ],
   "source": [
    "prediction_accuracy(image_test, model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks for best model\n",
    "runs = mlflow.search_runs(experiment_ids=experiment_id,\n",
    "                          order_by=['metrics.mae'], max_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1d438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8233bce6",
   "metadata": {},
   "source": [
    "# Segmentation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599480db",
   "metadata": {},
   "source": [
    "# MASK RCNN CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d290bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"E:\\Dissertation\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"data\\img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bfc39",
   "metadata": {},
   "source": [
    "## Extract Masks from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e724bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all the image paths stores in a list\n",
    "def get_img_paths(URL_dir):\n",
    "    img_paths = []\n",
    "    for path in os.listdir(URL_dir):\n",
    "        full_path = os.path.join(URL_dir, path)\n",
    "        if os.path.isfile(full_path):\n",
    "            img_paths.append(full_path)\n",
    "\n",
    "    return img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34762a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = get_img_paths(URL_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974129",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca326ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots each object on a copy of the image and attempts to remove noise\n",
    "\n",
    "def plot_mask(image, masks):\n",
    "    all_masks = list()\n",
    "    masks = masks.astype(int)\n",
    "    # print(masks.shape)\n",
    "    \n",
    "    for i in range(masks.shape[2]):\n",
    "        temp = np.copy(image)\n",
    "        for j in range(temp.shape[2]):\n",
    "            temp[:,:,j] = temp[:,:,j] * masks[:,:,i]\n",
    "        all_masks.append(temp)\n",
    "#         plt.figure(figsize=(8,8))\n",
    "#         plt.imshow(temp)\n",
    "    # print(f\"Number of masks: {len(all_masks)}\")\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665be4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overlay all masks onto a black canvas\n",
    "\n",
    "def combine_mask_objects(masks, shape):\n",
    "    canvas = np.zeros(shape, np.uint8)\n",
    "#     print(\"ORIGINAL CANVAS\")\n",
    "#     plt.figure(figsize=(8,8))\n",
    "#     plt.imshow(canvas)\n",
    "    for i, obj in enumerate(masks):\n",
    "        canvas = cv2.add(canvas, obj)\n",
    "#         plt.figure(figsize=(8,8))\n",
    "#         plt.imshow(canvas)\n",
    "    return canvas\n",
    "\n",
    "### Saves all masks for each image in a separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img_id, img, folder):\n",
    "    # checks if the img_id folder exists, if not create one\n",
    "    img = PIL.Image.fromarray(img, 'RGB')\n",
    "    img.save(f'{folder}/{img_id}.png')\n",
    "    return True       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8978a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(folder_name, img_paths):\n",
    "    # checks if the masks folder exists, if not creates one\n",
    "    print(os.getcwd())\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    for img_path in img_paths:\n",
    "        print(img_path)\n",
    "        img_id = Path(img_path).stem\n",
    "        print(img_id)\n",
    "        image = skimage.io.imread(img_path)\n",
    "        # check if image is in rgba format\n",
    "        if image.shape[2] > 3:\n",
    "            # convert to rgb\n",
    "            image = rgba2rgb(image)\n",
    "        # Run detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        # Visualize results\n",
    "        r = results[0]\n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], UNIQUE_LABELS, r['scores'])\n",
    "\n",
    "        masks = plot_mask(np.copy(image), r[\"masks\"])\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            processed_image = combine_mask_objects(masks, masks[0].shape)\n",
    "            print(masks)\n",
    "            save_image(i, processed_image, folder_name)\n",
    "        else:\n",
    "            print(f\"{img_id} has no masks!\")\n",
    "            save_image(img_id, image, folder_name)\n",
    "    return \"COMPLETE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee703fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join(\"E:\\Dissertation\")\n",
    "# os.chdir(path)\n",
    "# UNCOMMENT THIS LINE TO EXTRACT MASKS FROM DATATSET\n",
    "# BEWARE THIS PROCESS CAN TAKE HOURS TO FINISH!\n",
    "generate_masks(\"masks\", img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MASK_DIR = os.path.join(MAIN_PATH, 'data\\train_masks')\n",
    "DEV_SEEN_MASK_DIR = os.path.join(MAIN_PATH, 'data\\dev_seen_masks')\n",
    "DEV_UNSEEN_MASK_DIR = os.path.join(MAIN_PATH, 'data\\dev_unseen_masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce7b56",
   "metadata": {},
   "source": [
    "# 3. Create feature vector of object masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d13d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def extract_id(name):\n",
    "    filename = Path(name).stem\n",
    "    # print(filename)\n",
    "    return filename\n",
    "\n",
    "def generate_data(df, directory):\n",
    "    # dataset con\n",
    "    ids = df[\"id\"].to_numpy()\n",
    "    labels = df[\"label\"].to_numpy()\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        print(root)\n",
    "        count = 0\n",
    "        for file in tqdm(files):\n",
    "            # print(f\"IMAGE: {file}\")\n",
    "            # get full path of the mask\n",
    "            full_path = os.path.join(root, file)\n",
    "            # extract the image id from file\n",
    "            img_id = extract_id(file)\n",
    "            # get tensor of the mask\n",
    "            image = Image.open(full_path)\n",
    "            image = image.resize((224, 224))\n",
    "            image_vect = np.asarray(image)\n",
    "            # print(image_vect.shape)\n",
    "            # find the index for the image_id\n",
    "            idx = np.where(ids == int(img_id))[0][0]\n",
    "            # print(df.iloc[idx])\n",
    "\n",
    "            # get the label for that image_id\n",
    "            label = labels[idx]\n",
    "\n",
    "            # Append features and labels to data lists\n",
    "            X.append(image_vect)\n",
    "            y.append(label)\n",
    "        break\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_data(TRAIN_DF, TRAIN_MASK_DIR)\n",
    "\n",
    "# uncomment to generate training data for dev masks\n",
    "# X, y = generate_data(DEV_SEEN_DF, DEV_SEEN_MASK_DIR)\n",
    "# X, y = generate_data(DEV_UNSEEN_DF, DEV_UNSEEN_MASK_DIR)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c5b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335db64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd41cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29721ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad216f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43996e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc119a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1364a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a01fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704716c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb928e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28e2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b90d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b52e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4a541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335092d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd45548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649833bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a3ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b8c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
