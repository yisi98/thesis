{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737f5f14",
   "metadata": {},
   "source": [
    "# Dissertation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3073d9",
   "metadata": {},
   "source": [
    "## Research into the techniques and methods to achieve state of the art accuracy in flower species identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c97bbfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4d6984ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###imports###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from scipy import io\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "#import skimage.io\n",
    "import random\n",
    "import PIL\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4fa11d15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a43f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eabfba54",
   "metadata": {},
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ecf1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes all outputs be in float format rather than exponentials\n",
    "#np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c860a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "IMG_SIZE = 224 #this parameter sets image dimensions as 50*50\n",
    "DATE = datetime.datetime.now().strftime('%d-%b-%Y')\n",
    "MODEL_PATH = f'models/{DATE}/'\n",
    "MODEL_NAME = 'FlowerClassifierTrial.model'.format(int(time.time()))\n",
    "log_dir=f'logs\\\\{MODEL_NAME}'\n",
    "TENSORBOARD = TensorBoard(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "48c687c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "16ed7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbdd12",
   "metadata": {},
   "source": [
    "# Load Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f1c4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Directory for MacBook\n",
    "# mac_URL = '/Volumes/T7/Uni/Github/thesis/flowerDataset/Flower'\n",
    "# mac_labels = '/Volumes/T7/Uni/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "# mac_dataSplit = '/Volumes/T7/Uni/Github/thesis/flowerDataset/setid.mat'\n",
    "# mac_loaded_images = '/Volumes/T7/Uni/loaded_images.npy'\n",
    "# mac_image_train='/Volumes/T7/Uni/image_train.npy'\n",
    "# mac_label_train = '/Volumes/T7/Uni/label_train.npy'\n",
    "# mac_image_test = '/Volumes/T7/Uni/image_test.npy'\n",
    "# mac_label_test = '/Volumes/T7/Uni/label_test.npy'\n",
    "# mac_image_val = '/Volumes/T7/Uni/image_val.npy'\n",
    "# mac_label_val = '/Volumes/T7/Uni/label_val.npy'\n",
    "\n",
    "# ###Directory for PC\n",
    "# pc_URL = 'E:/Github/thesis/flowerDataset/Flower'\n",
    "# pc_labels = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "# pc_dataSplit = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "# pc_loaded_images = 'E:/Dissertation/data/loaded_images.npy'\n",
    "# pc_image_train = 'E:/Dissertation/data/image_train.npy'\n",
    "# pc_label_train = 'E:/Dissertation/data/label_train.npy'\n",
    "# pc_image_test = 'E:/Dissertation/data/image_test.npy'\n",
    "# pc_label_test = 'E:/Dissertation/data/label_test.npy'\n",
    "# pc_image_val = 'E:/Dissertation/data/image_val.npy'\n",
    "# pc_label_val = 'E:/Dissertation/data/label_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1180309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories():\n",
    "    if os.path.exists(mac_URL) and os.path.exists(mac_labels) and os.path.exists(\n",
    "            mac_dataSplit) and os.path.exists(mac_loaded_images):\n",
    "        return mac_URL, mac_labels, mac_dataSplit, mac_loaded_images, mac_image_train, mac_label_train, mac_image_test, mac_label_test,mac_image_val, mac_label_val \n",
    "    else:\n",
    "        return pc_URL, pc_labels, pc_dataSplit, pc_loaded_images, pc_image_train, pc_label_train, pc_image_test, pc_label_test, pc_image_val, pc_label_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04ec1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL_dir, labels_dir, dataSplit_dir, loaded_images_dir, image_train_dir, label_train_dir, image_test_dir, label_test_dir, image_val_dir, label_val_dir = get_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "af5a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_dir = 'E:/Github/thesis/flowerDataset/Flower'\n",
    "labels_dir = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "dataSplit_dir = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "loaded_images_dir = 'E:/Dissertation/data/loaded_images.npy'\n",
    "image_train_dir ='E:/Dissertation/data/image_train.npy'\n",
    "label_train_dir = 'E:/Dissertation/data/label_train.npy'\n",
    "image_test_dir = 'E:/Dissertation/data/image_test.npy'\n",
    "label_test_dir = 'E:/Dissertation/data/label_test.npy'\n",
    "image_val_dir = 'E:/Dissertation/data/image_val.npy'\n",
    "label_val_dir = 'E:/Dissertation/data/label_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "591cf84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Github/thesis/flowerDataset/imagelabels.mat\n"
     ]
    }
   ],
   "source": [
    "print(labels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0c1a",
   "metadata": {},
   "source": [
    "### Load DataSet and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "17b53f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = io.loadmat(labels_dir)\n",
    "data_labels = data_labels.items()\n",
    "data_labels = list(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c2554af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d41c40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[1][3][0] #loads the labels that is stored in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a0b6511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 77, 77, ..., 62, 62, 62], dtype=uint8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ed0a2854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b102eefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102], dtype=uint8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_LABELS = np.unique(labels)\n",
    "UNIQUE_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ff41223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got loading files code from link below\n",
    "#https://stackoverflow.com/questions/30230592/loading-all-images-using-imread-from-a-given-folder\n",
    "#Function gets all the images loaded up\n",
    "def load_images_from_folder(URL):\n",
    "    images = []\n",
    "    for filename in os.listdir(URL):\n",
    "        #img = cv2.imread(os.path.join(URL,filename))\n",
    "        img = cv2.resize(cv2.imread(os.path.join(URL,filename), cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "        img = np.reshape(img,[IMG_SIZE,IMG_SIZE,3])\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    np.save(loaded_images_dir, images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "563d6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to run this cell if image already loaded.\n",
    "#load_images_from_folder(URL_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2b32b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if images are already loaded, just load the file here rather than running function again.\n",
    "image_data = np.load(loaded_images_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0969fb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8189, 224, 224, 3)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows the shape of the image_data, the number of images, the dimensions and number of colour channels\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f5eec5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4356b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine image with labels\n",
    "def randomised_dataset(image_data, labels):\n",
    "    np.random.seed(8)\n",
    "    indices = np.arange(image_data.shape[0])\n",
    "    print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    image_data = image_data[indices]\n",
    "    labels = labels[indices]\n",
    "    return image_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "965b929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_data, labels = randomised_dataset(image_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5cebed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8189, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4efd03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Label Encoder\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "converted_labels = np.array(mlb.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "09ba97fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b6fb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(labels):\n",
    "    new_labels = []\n",
    "    for i in range(len(labels)):\n",
    "        new_labels.insert(i, [labels[i]])\n",
    "    return np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7430f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted_labels = convert_label(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e40f31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "52f70b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cf733688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6dc2e",
   "metadata": {},
   "source": [
    "# Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64e30768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citation code from https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "#splitting the image dataset into the ratio for training, validation and testing data\n",
    "def split_data(image_data, converted_labels):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\n",
    "    image_train, image_test, label_train, label_test = train_test_split(image_data, converted_labels, test_size=1 - train_ratio, random_state=42)\n",
    "\n",
    "    #the test from previous line which is 25% of dataset is passed into the line below to be \n",
    "    #further split into 15% for validation and 10% for testing\n",
    "\n",
    "    image_val, image_test, label_val, label_test = train_test_split(image_test, label_test, test_size=test_ratio/(test_ratio + validation_ratio),random_state=42) \n",
    "\n",
    "    print('image_train',image_train.shape)\n",
    "    print('label_train',label_train.shape)\n",
    "    print('image_test',image_test.shape)\n",
    "    print('label_test',label_test.shape)\n",
    "    print('image_val', image_val.shape)\n",
    "    print('label_val', label_val.shape)\n",
    "\n",
    "    np.save(image_train_dir, image_train)\n",
    "    np.save(label_train_dir, label_train)\n",
    "    np.save(image_test_dir, image_test)\n",
    "    np.save(label_test_dir, label_test)\n",
    "    np.save(image_val_dir, image_val)\n",
    "    np.save(label_val_dir, label_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3d985813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train (6141, 224, 224, 3)\n",
      "label_train (6141, 102)\n",
      "image_test (820, 224, 224, 3)\n",
      "label_test (820, 102)\n",
      "image_val (1228, 224, 224, 3)\n",
      "label_val (1228, 102)\n"
     ]
    }
   ],
   "source": [
    "split_data(image_data,converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aee55642",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.load(image_train_dir, allow_pickle=True)\n",
    "image_test = np.load(image_test_dir, allow_pickle=True)\n",
    "label_train = np.load(label_train_dir, allow_pickle=True)\n",
    "label_test = np.load(label_test_dir, allow_pickle=True)\n",
    "image_val = np.load(image_val_dir, allow_pickle=True)\n",
    "label_val = np.load(label_val_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "606a621e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 224, 224, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "952d08fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a1187",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff6920dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the images to the range of 0 - 1.0 \n",
    "image_train = image_train/255.0\n",
    "image_test = image_test/255.0\n",
    "image_val = image_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e1bb389a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 224, 224, 3)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4e534027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 102)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "312de917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 102)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "90564584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8328cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "55d0ae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of classes/ flowers\n",
    "NUM_CLASSES = 102\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "040ac502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 102)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1925a731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18204d6a",
   "metadata": {},
   "source": [
    "# Custom CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "dc375b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of model\n",
    "name_model = 'test-{}'.format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "81c56ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 100\n",
    "verbose= 2\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4a6597d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(act):\n",
    "    # tf.reset_default_graph()\n",
    "    model = Sequential()\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # INPUT LAYER\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation(act))\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # HIDDEN LAYER 1\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # HIDDEN LAYER 2\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 3\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # Fully Connected\n",
    "    model.add(Flatten()) # converts the 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.25)) # reduces overfitting\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6803cb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_cnn_model(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "53d5fff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_42 (Activation)   (None, 222, 222, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 220, 220, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 220, 220, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 110, 110, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 108, 108, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 54, 54, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 24, 24, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               9437440   \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 102)               26214     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 102)               0         \n",
      "=================================================================\n",
      "Total params: 9,863,238\n",
      "Trainable params: 9,862,278\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b2c8b375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit(image_train, label_train, batch_size=64, epochs=100, validation_data=(image_val, label_val), verbose=1, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a7e4887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "#model.save(f'{MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ff2059af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "#model =  tf.keras.models.load_model(f'{MODEL_PATH}')\n",
    "#model = tf.keras.models.load_model(f'E:/GoogleSync/Masters/Dissertation/models/24-Jun-2021') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170a1d3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "27a9ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c30171a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/08/08 15:13:08 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0590s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0120s vs `on_train_batch_end` time: 0.0590s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 - 15s - loss: 4.9109 - accuracy: 0.0259 - val_loss: 5.8558 - val_accuracy: 0.0399\n",
      "Epoch 2/100\n",
      "768/768 - 10s - loss: 4.5779 - accuracy: 0.0321 - val_loss: 4.5908 - val_accuracy: 0.0318\n",
      "Epoch 3/100\n",
      "768/768 - 10s - loss: 4.5418 - accuracy: 0.0353 - val_loss: 4.5874 - val_accuracy: 0.0301\n",
      "Epoch 4/100\n",
      "768/768 - 10s - loss: 4.5126 - accuracy: 0.0365 - val_loss: 7.5044 - val_accuracy: 0.0358\n",
      "Epoch 5/100\n",
      "768/768 - 10s - loss: 4.5616 - accuracy: 0.0319 - val_loss: 4.6321 - val_accuracy: 0.0334\n",
      "Epoch 6/100\n",
      "768/768 - 10s - loss: 4.5153 - accuracy: 0.0331 - val_loss: 4.5235 - val_accuracy: 0.0293\n",
      "Epoch 7/100\n",
      "768/768 - 10s - loss: 4.5063 - accuracy: 0.0314 - val_loss: 4.5218 - val_accuracy: 0.0293\n",
      "Epoch 8/100\n",
      "768/768 - 10s - loss: 4.4938 - accuracy: 0.0321 - val_loss: 4.5215 - val_accuracy: 0.0375\n",
      "Epoch 9/100\n",
      "768/768 - 10s - loss: 4.4932 - accuracy: 0.0305 - val_loss: 4.5214 - val_accuracy: 0.0366\n",
      "Epoch 10/100\n",
      "768/768 - 10s - loss: 4.4865 - accuracy: 0.0337 - val_loss: 4.5252 - val_accuracy: 0.0366\n",
      "Epoch 11/100\n",
      "768/768 - 10s - loss: 4.5028 - accuracy: 0.0324 - val_loss: 4.5472 - val_accuracy: 0.0366\n",
      "Epoch 12/100\n",
      "768/768 - 10s - loss: 4.4940 - accuracy: 0.0313 - val_loss: 4.5214 - val_accuracy: 0.0366\n",
      "Epoch 13/100\n",
      "768/768 - 10s - loss: 4.4928 - accuracy: 0.0329 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 14/100\n",
      "768/768 - 10s - loss: 4.4968 - accuracy: 0.0322 - val_loss: 4.5399 - val_accuracy: 0.0366\n",
      "Epoch 15/100\n",
      "768/768 - 11s - loss: 4.4980 - accuracy: 0.0327 - val_loss: 4.6436 - val_accuracy: 0.0350\n",
      "Epoch 16/100\n",
      "768/768 - 11s - loss: 4.4863 - accuracy: 0.0331 - val_loss: 4.5224 - val_accuracy: 0.0366\n",
      "Epoch 17/100\n",
      "768/768 - 11s - loss: 4.4882 - accuracy: 0.0326 - val_loss: 4.5225 - val_accuracy: 0.0366\n",
      "Epoch 18/100\n",
      "768/768 - 12s - loss: 4.4910 - accuracy: 0.0316 - val_loss: 4.5226 - val_accuracy: 0.0366\n",
      "Epoch 19/100\n",
      "768/768 - 13s - loss: 4.4916 - accuracy: 0.0316 - val_loss: 4.5227 - val_accuracy: 0.0366\n",
      "Epoch 20/100\n",
      "768/768 - 11s - loss: 4.4877 - accuracy: 0.0324 - val_loss: 4.5227 - val_accuracy: 0.0366\n",
      "Epoch 21/100\n",
      "768/768 - 11s - loss: 4.4856 - accuracy: 0.0324 - val_loss: 4.5227 - val_accuracy: 0.0366\n",
      "Epoch 22/100\n",
      "768/768 - 11s - loss: 4.5019 - accuracy: 0.0319 - val_loss: 4.5259 - val_accuracy: 0.0366\n",
      "Epoch 23/100\n",
      "768/768 - 11s - loss: 4.4983 - accuracy: 0.0318 - val_loss: 4.5229 - val_accuracy: 0.0366\n",
      "Epoch 24/100\n",
      "768/768 - 11s - loss: 4.4934 - accuracy: 0.0316 - val_loss: 4.5230 - val_accuracy: 0.0366\n",
      "Epoch 25/100\n",
      "768/768 - 12s - loss: 4.5026 - accuracy: 0.0306 - val_loss: 4.6446 - val_accuracy: 0.0358\n",
      "Epoch 26/100\n",
      "768/768 - 11s - loss: 4.4941 - accuracy: 0.0316 - val_loss: 4.5229 - val_accuracy: 0.0366\n",
      "Epoch 27/100\n",
      "768/768 - 11s - loss: 4.4931 - accuracy: 0.0316 - val_loss: 4.5230 - val_accuracy: 0.0366\n",
      "Epoch 28/100\n",
      "768/768 - 11s - loss: 4.4961 - accuracy: 0.0313 - val_loss: 4.5230 - val_accuracy: 0.0366\n",
      "Epoch 29/100\n",
      "768/768 - 11s - loss: 4.4968 - accuracy: 0.0319 - val_loss: 5.0334 - val_accuracy: 0.0375\n",
      "Epoch 30/100\n",
      "768/768 - 11s - loss: 4.4946 - accuracy: 0.0318 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 31/100\n",
      "768/768 - 11s - loss: 4.4946 - accuracy: 0.0319 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 32/100\n",
      "768/768 - 12s - loss: 4.4927 - accuracy: 0.0318 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 33/100\n",
      "768/768 - 12s - loss: 4.4926 - accuracy: 0.0311 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 34/100\n",
      "768/768 - 11s - loss: 4.4912 - accuracy: 0.0321 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 35/100\n",
      "768/768 - 11s - loss: 4.4921 - accuracy: 0.0318 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 36/100\n",
      "768/768 - 11s - loss: 4.4905 - accuracy: 0.0322 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 37/100\n",
      "768/768 - 11s - loss: 4.4933 - accuracy: 0.0311 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 38/100\n",
      "768/768 - 11s - loss: 4.4905 - accuracy: 0.0309 - val_loss: 4.5234 - val_accuracy: 0.0366\n",
      "Epoch 39/100\n",
      "768/768 - 10s - loss: 4.4895 - accuracy: 0.0324 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 40/100\n",
      "768/768 - 10s - loss: 4.4946 - accuracy: 0.0321 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 41/100\n",
      "768/768 - 10s - loss: 4.4883 - accuracy: 0.0327 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 42/100\n",
      "768/768 - 10s - loss: 4.4909 - accuracy: 0.0322 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 43/100\n",
      "768/768 - 10s - loss: 4.4882 - accuracy: 0.0327 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 44/100\n",
      "768/768 - 10s - loss: 4.4902 - accuracy: 0.0324 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 45/100\n",
      "768/768 - 11s - loss: 4.4950 - accuracy: 0.0318 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 46/100\n",
      "768/768 - 11s - loss: 4.4913 - accuracy: 0.0321 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 47/100\n",
      "768/768 - 11s - loss: 4.4940 - accuracy: 0.0306 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 48/100\n",
      "768/768 - 11s - loss: 4.4904 - accuracy: 0.0305 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 49/100\n",
      "768/768 - 12s - loss: 4.4996 - accuracy: 0.0326 - val_loss: 4.5230 - val_accuracy: 0.0366\n",
      "Epoch 50/100\n",
      "768/768 - 11s - loss: 4.4930 - accuracy: 0.0313 - val_loss: 4.5230 - val_accuracy: 0.0366\n",
      "Epoch 51/100\n",
      "768/768 - 11s - loss: 4.4913 - accuracy: 0.0321 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 52/100\n",
      "768/768 - 11s - loss: 4.4891 - accuracy: 0.0324 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 53/100\n",
      "768/768 - 11s - loss: 4.4908 - accuracy: 0.0301 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 54/100\n",
      "768/768 - 11s - loss: 4.4898 - accuracy: 0.0322 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 55/100\n",
      "768/768 - 11s - loss: 4.4874 - accuracy: 0.0329 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 56/100\n",
      "768/768 - 12s - loss: 4.4905 - accuracy: 0.0321 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 57/100\n",
      "768/768 - 11s - loss: 4.4906 - accuracy: 0.0300 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 58/100\n",
      "768/768 - 11s - loss: 4.4904 - accuracy: 0.0321 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 59/100\n",
      "768/768 - 13s - loss: 4.4890 - accuracy: 0.0321 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 60/100\n",
      "768/768 - 11s - loss: 4.4937 - accuracy: 0.0324 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 61/100\n",
      "768/768 - 11s - loss: 4.4981 - accuracy: 0.0318 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 62/100\n",
      "768/768 - 11s - loss: 4.4915 - accuracy: 0.0326 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 63/100\n",
      "768/768 - 12s - loss: 4.4956 - accuracy: 0.0321 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 64/100\n",
      "768/768 - 11s - loss: 4.4909 - accuracy: 0.0314 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 65/100\n",
      "768/768 - 10s - loss: 4.4895 - accuracy: 0.0319 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 66/100\n",
      "768/768 - 11s - loss: 4.4899 - accuracy: 0.0321 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 67/100\n",
      "768/768 - 11s - loss: 4.4876 - accuracy: 0.0326 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 68/100\n",
      "768/768 - 11s - loss: 4.4876 - accuracy: 0.0326 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 69/100\n",
      "768/768 - 10s - loss: 4.4909 - accuracy: 0.0295 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 70/100\n",
      "768/768 - 10s - loss: 4.4896 - accuracy: 0.0326 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 71/100\n",
      "768/768 - 10s - loss: 4.4921 - accuracy: 0.0322 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 72/100\n",
      "768/768 - 10s - loss: 4.4924 - accuracy: 0.0318 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 73/100\n",
      "768/768 - 10s - loss: 4.4921 - accuracy: 0.0314 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 74/100\n",
      "768/768 - 10s - loss: 4.4867 - accuracy: 0.0308 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 75/100\n",
      "768/768 - 10s - loss: 4.4888 - accuracy: 0.0300 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 76/100\n",
      "768/768 - 10s - loss: 4.4901 - accuracy: 0.0324 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 77/100\n",
      "768/768 - 11s - loss: 4.4887 - accuracy: 0.0326 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 78/100\n",
      "768/768 - 11s - loss: 4.4875 - accuracy: 0.0327 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "Epoch 79/100\n",
      "768/768 - 11s - loss: 4.4881 - accuracy: 0.0327 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 80/100\n",
      "768/768 - 12s - loss: 4.4865 - accuracy: 0.0329 - val_loss: 4.5233 - val_accuracy: 0.0366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "768/768 - 12s - loss: 4.4988 - accuracy: 0.0324 - val_loss: 4.5228 - val_accuracy: 0.0366\n",
      "Epoch 82/100\n",
      "768/768 - 12s - loss: 4.4898 - accuracy: 0.0322 - val_loss: 4.5230 - val_accuracy: 0.0366\n",
      "Epoch 83/100\n",
      "768/768 - 11s - loss: 4.4911 - accuracy: 0.0322 - val_loss: 4.5225 - val_accuracy: 0.0366\n",
      "Epoch 84/100\n",
      "768/768 - 11s - loss: 4.4862 - accuracy: 0.0329 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 85/100\n",
      "768/768 - 11s - loss: 4.4898 - accuracy: 0.0322 - val_loss: 4.5226 - val_accuracy: 0.0366\n",
      "Epoch 86/100\n",
      "768/768 - 11s - loss: 4.4898 - accuracy: 0.0309 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 87/100\n",
      "768/768 - 12s - loss: 4.4876 - accuracy: 0.0327 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 88/100\n",
      "768/768 - 11s - loss: 4.4883 - accuracy: 0.0318 - val_loss: 4.5231 - val_accuracy: 0.0366\n",
      "Epoch 89/100\n",
      "768/768 - 11s - loss: 4.4936 - accuracy: 0.0314 - val_loss: 4.5255 - val_accuracy: 0.0366\n",
      "Epoch 90/100\n",
      "768/768 - 11s - loss: 4.4921 - accuracy: 0.0331 - val_loss: 4.5234 - val_accuracy: 0.0366\n",
      "Epoch 91/100\n",
      "768/768 - 11s - loss: 4.4849 - accuracy: 0.0335 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 92/100\n",
      "768/768 - 11s - loss: 4.4905 - accuracy: 0.0326 - val_loss: 4.5234 - val_accuracy: 0.0366\n",
      "Epoch 93/100\n",
      "768/768 - 11s - loss: 4.4879 - accuracy: 0.0326 - val_loss: 4.5234 - val_accuracy: 0.0366\n",
      "Epoch 94/100\n",
      "768/768 - 12s - loss: 4.4886 - accuracy: 0.0324 - val_loss: 4.5235 - val_accuracy: 0.0366\n",
      "Epoch 95/100\n",
      "768/768 - 12s - loss: 4.4901 - accuracy: 0.0319 - val_loss: 4.5235 - val_accuracy: 0.0366\n",
      "Epoch 96/100\n",
      "768/768 - 14s - loss: 4.4888 - accuracy: 0.0314 - val_loss: 4.5233 - val_accuracy: 0.0366\n",
      "Epoch 97/100\n",
      "768/768 - 13s - loss: 4.4882 - accuracy: 0.0326 - val_loss: 4.5234 - val_accuracy: 0.0366\n",
      "Epoch 98/100\n",
      "768/768 - 12s - loss: 4.4886 - accuracy: 0.0326 - val_loss: 4.5236 - val_accuracy: 0.0366\n",
      "Epoch 99/100\n",
      "768/768 - 11s - loss: 4.4871 - accuracy: 0.0314 - val_loss: 4.5234 - val_accuracy: 0.0366\n",
      "Epoch 100/100\n",
      "768/768 - 11s - loss: 4.4859 - accuracy: 0.0308 - val_loss: 4.5232 - val_accuracy: 0.0366\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\yisi9\\AppData\\Local\\Temp\\tmpjet1vn5t\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\yisi9\\AppData\\Local\\Temp\\tmpjet1vn5t\\model\\data\\model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 - 1s - loss: 4.5118 - accuracy: 0.0293\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"custom-model-tests\"\n",
    "RUN_NAME = \"eight_batch_default_cnn_colour\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_param(\"activation function\", act)\n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        #mlflow.keras.log_model(model, \"standardCNN\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ef9ac114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 15ms/step - loss: 4.5118 - accuracy: 0.0293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.511775493621826, 0.02926829271018505]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function that returns loss value & metrics values\n",
    "model.evaluate(image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ec96c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31c612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86168ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(image_test, model):\n",
    "    #going through each of the test images\n",
    "    count = 0\n",
    "    for i in range(len(image_test)): #going through all test images\n",
    "        img = image_test[i] #load in the image\n",
    "        \n",
    "        img = img.reshape(-1, 100, 100, 3) #get it in the right shape for model.predict\n",
    "        \n",
    "        prediction = model.predict(img).flatten() #using model predict to get what it thinks is the answer\n",
    "        \n",
    "        # prediction.astype(np.int) #converts predictions to integers\n",
    "        \n",
    "        pred_index = np.argmax(prediction) # grabs the INDEX of the best prediction\n",
    "        \n",
    "        max_pred = max(prediction) # grabs the SCORE of best prediction\n",
    "        # print(prediction)\n",
    "        # print(pred_index)\n",
    "        \n",
    "        test_label_value = label_test[i][pred_index] # gets the actual test label at the same index\n",
    "        # print(test_label_value)\n",
    "        if test_label_value == 1:\n",
    "            #print(\"CORRECT PREDICTION\")\n",
    "            count += 1\n",
    "            \n",
    "#         # FIND THE INDEX WHERE THE VALUE = 1\n",
    "#         for j in range(len(predicted_label)):\n",
    "#             if predicted_label[j] == 1 : #find the point when it finds 1 \n",
    "#                 if label_test[i][j] == 1: #check if at this exact point is also 1  whic means correct prediction\n",
    "#                     count+=1 #increase number of correct count by 1\n",
    "#                     print(count)\n",
    "#                     break\n",
    "    print(f\"num correct - {count}\")\n",
    "    accuracy = (count/len(label_test))*100\n",
    "    per_symbol = '%'\n",
    "    print(f'accuracy is {accuracy} {per_symbol}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446de09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad03f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_accuracy(image_test, model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1d438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
