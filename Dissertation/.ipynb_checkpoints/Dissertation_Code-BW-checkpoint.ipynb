{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737f5f14",
   "metadata": {},
   "source": [
    "# Dissertation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3073d9",
   "metadata": {},
   "source": [
    "## Research into the techniques and methods to achieve state of the art accuracy in flower species identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6984ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###imports###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from scipy import io\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "#import skimage.io\n",
    "import random\n",
    "import PIL\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa11d15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a43f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbda12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"file:///E:/Github/thesis/Dissertation/mlruns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed53ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "    \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecf1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes all outputs be in float format rather than exponentials\n",
    "#np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c860a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "IMG_SIZE = 224 #this parameter sets image dimensions as 50*50\n",
    "DATE = datetime.datetime.now().strftime('%d-%b-%Y')\n",
    "MODEL_PATH = f'models/{DATE}/'\n",
    "MODEL_NAME = 'FlowerClassifierTrial.model'.format(int(time.time()))\n",
    "log_dir=f'logs\\\\{MODEL_NAME}'\n",
    "TENSORBOARD = TensorBoard(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c687c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ed7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbdd12",
   "metadata": {},
   "source": [
    "# Load Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1c4c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Directory for MacBook\n",
    "# mac_URL = '/Volumes/T7/Uni/Github/thesis/flowerDataset/Flower'\n",
    "# mac_labels = '/Volumes/T7/Uni/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "# mac_dataSplit = '/Volumes/T7/Uni/Github/thesis/flowerDataset/setid.mat'\n",
    "# mac_loaded_images = '/Volumes/T7/Uni/loaded_images.npy'\n",
    "# mac_image_train='/Volumes/T7/Uni/image_train.npy'\n",
    "# mac_label_train = '/Volumes/T7/Uni/label_train.npy'\n",
    "# mac_image_test = '/Volumes/T7/Uni/image_test.npy'\n",
    "# mac_label_test = '/Volumes/T7/Uni/label_test.npy'\n",
    "# mac_image_val = '/Volumes/T7/Uni/image_val.npy'\n",
    "# mac_label_val = '/Volumes/T7/Uni/label_val.npy'\n",
    "\n",
    "# ###Directory for PC\n",
    "# pc_URL = 'E:/Github/thesis/flowerDataset/Flower'\n",
    "# pc_labels = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "# pc_dataSplit = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "# pc_loaded_images = 'E:/Dissertation/data/loaded_images.npy'\n",
    "# pc_image_train = 'E:/Dissertation/data/image_train.npy'\n",
    "# pc_label_train = 'E:/Dissertation/data/label_train.npy'\n",
    "# pc_image_test = 'E:/Dissertation/data/image_test.npy'\n",
    "# pc_label_test = 'E:/Dissertation/data/label_test.npy'\n",
    "# pc_image_val = 'E:/Dissertation/data/image_val.npy'\n",
    "# pc_label_val = 'E:/Dissertation/data/label_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1180309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_directories():\n",
    "    if os.path.exists(mac_URL) and os.path.exists(mac_labels) and os.path.exists(\n",
    "            mac_dataSplit) and os.path.exists(mac_loaded_images):\n",
    "        return mac_URL, mac_labels, mac_dataSplit, mac_loaded_images, mac_image_train, mac_label_train, mac_image_test, mac_label_test,mac_image_val, mac_label_val \n",
    "    else:\n",
    "        return pc_URL, pc_labels, pc_dataSplit, pc_loaded_images, pc_image_train, pc_label_train, pc_image_test, pc_label_test, pc_image_val, pc_label_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04ec1614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL_dir, labels_dir, dataSplit_dir, loaded_images_dir, image_train_dir, label_train_dir, image_test_dir, label_test_dir, image_val_dir, label_val_dir = get_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af5a630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_dir = 'E:/Github/thesis/flowerDataset/Flower'\n",
    "labels_dir = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "dataSplit_dir = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "loaded_images_dir = 'E:/Dissertation/data/loaded_images.npy'\n",
    "image_train_dir ='E:/Dissertation/data/image_train.npy'\n",
    "label_train_dir = 'E:/Dissertation/data/label_train.npy'\n",
    "image_test_dir = 'E:/Dissertation/data/image_test.npy'\n",
    "label_test_dir = 'E:/Dissertation/data/label_test.npy'\n",
    "image_val_dir = 'E:/Dissertation/data/image_val.npy'\n",
    "label_val_dir = 'E:/Dissertation/data/label_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "591cf84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/Github/thesis/flowerDataset/imagelabels.mat\n"
     ]
    }
   ],
   "source": [
    "print(labels_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f0c1a",
   "metadata": {},
   "source": [
    "### Load DataSet and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17b53f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = io.loadmat(labels_dir)\n",
    "data_labels = data_labels.items()\n",
    "data_labels = list(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c2554af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d41c40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[1][3][0] #loads the labels that is stored in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a0b6511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([77, 77, 77, ..., 62, 62, 62], dtype=uint8)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed0a2854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b102eefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102], dtype=uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNIQUE_LABELS = np.unique(labels)\n",
    "UNIQUE_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ff41223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got loading files code from link below\n",
    "#https://stackoverflow.com/questions/30230592/loading-all-images-using-imread-from-a-given-folder\n",
    "#Function gets all the images loaded up\n",
    "def load_images_from_folder(URL):\n",
    "    images = []\n",
    "    for filename in os.listdir(URL):\n",
    "        #img = cv2.imread(os.path.join(URL,filename))\n",
    "        img = cv2.resize(cv2.imread(os.path.join(URL,filename), cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "        img = np.reshape(img,[IMG_SIZE,IMG_SIZE,3])\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    np.save(loaded_images_dir, images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "563d6ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to run this cell if image already loaded.\n",
    "#load_images_from_folder(URL_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b32b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if images are already loaded, just load the file here rather than running function again.\n",
    "image_data = np.load(loaded_images_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0969fb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8189, 224, 224, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows the shape of the image_data, the number of images, the dimensions and number of colour channels\n",
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5eec5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4356b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine image with labels\n",
    "def randomised_dataset(image_data, labels):\n",
    "    np.random.seed(8)\n",
    "    indices = np.arange(image_data.shape[0])\n",
    "    print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    image_data = image_data[indices]\n",
    "    labels = labels[indices]\n",
    "    return image_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "965b929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_data, labels = randomised_dataset(image_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cebed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8189, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4efd03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Label Encoder\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "converted_labels = np.array(mlb.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09ba97fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b6fb4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(labels):\n",
    "    new_labels = []\n",
    "    for i in range(len(labels)):\n",
    "        new_labels.insert(i, [labels[i]])\n",
    "    return np.array(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7430f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted_labels = convert_label(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e40f31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52f70b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf733688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6dc2e",
   "metadata": {},
   "source": [
    "# Training and Testing Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64e30768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citation code from https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "#splitting the image dataset into the ratio for training, validation and testing data\n",
    "def split_data(image_data, converted_labels):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\n",
    "    image_train, image_test, label_train, label_test = train_test_split(image_data, converted_labels, test_size=1 - train_ratio, random_state=42)\n",
    "\n",
    "    #the test from previous line which is 25% of dataset is passed into the line below to be \n",
    "    #further split into 15% for validation and 10% for testing\n",
    "\n",
    "    image_val, image_test, label_val, label_test = train_test_split(image_test, label_test, test_size=test_ratio/(test_ratio + validation_ratio),random_state=42) \n",
    "\n",
    "    print('image_train',image_train.shape)\n",
    "    print('label_train',label_train.shape)\n",
    "    print('image_test',image_test.shape)\n",
    "    print('label_test',label_test.shape)\n",
    "    print('image_val', image_val.shape)\n",
    "    print('label_val', label_val.shape)\n",
    "\n",
    "    np.save(image_train_dir, image_train)\n",
    "    np.save(label_train_dir, label_train)\n",
    "    np.save(image_test_dir, image_test)\n",
    "    np.save(label_test_dir, label_test)\n",
    "    np.save(image_val_dir, image_val)\n",
    "    np.save(label_val_dir, label_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d985813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train (6141, 224, 224, 3)\n",
      "label_train (6141, 102)\n",
      "image_test (820, 224, 224, 3)\n",
      "label_test (820, 102)\n",
      "image_val (1228, 224, 224, 3)\n",
      "label_val (1228, 102)\n"
     ]
    }
   ],
   "source": [
    "split_data(image_data,converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aee55642",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.load(image_train_dir, allow_pickle=True)\n",
    "image_test = np.load(image_test_dir, allow_pickle=True)\n",
    "label_train = np.load(label_train_dir, allow_pickle=True)\n",
    "label_test = np.load(label_test_dir, allow_pickle=True)\n",
    "image_val = np.load(image_val_dir, allow_pickle=True)\n",
    "label_val = np.load(label_val_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "606a621e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 224, 224, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "952d08fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_val[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a1187",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff6920dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the images to the range of 0 - 1.0 \n",
    "image_train = image_train/255.0\n",
    "image_test = image_test/255.0\n",
    "image_val = image_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1bb389a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 224, 224, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e534027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 102)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "312de917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 102)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90564584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8328cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55d0ae0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of classes/ flowers\n",
    "NUM_CLASSES = 102\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "040ac502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 102)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1925a731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18204d6a",
   "metadata": {},
   "source": [
    "# Custom CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc375b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of model\n",
    "name_model = 'test-{}'.format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "81c56ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 300\n",
    "verbose= 2\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a6597d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(act):\n",
    "    # tf.reset_default_graph()\n",
    "    model = Sequential()\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # INPUT LAYER\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "    model.add(Activation(act))\n",
    "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # HIDDEN LAYER 1\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # HIDDEN LAYER 2\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 3\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 4\n",
    "    model.add(Conv2D(256, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # Fully Connected\n",
    "    model.add(Flatten()) # converts the 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(act))\n",
    "    model.add(Dropout(0.25)) # reduces overfitting\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6803cb31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = create_cnn_model(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53d5fff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 222, 222, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 220, 220, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 220, 220, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 110, 110, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 108, 108, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 54, 54, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 24, 24, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               9437440   \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 102)               26214     \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 102)               0         \n",
      "=================================================================\n",
      "Total params: 9,863,238\n",
      "Trainable params: 9,862,278\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2c8b375",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#history = model.fit(image_train, label_train, batch_size=64, epochs=100, validation_data=(image_val, label_val), verbose=1, callbacks = [tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a7e4887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "#model.save(f'{MODEL_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff2059af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model\n",
    "#model =  tf.keras.models.load_model(f'{MODEL_PATH}')\n",
    "#model = tf.keras.models.load_model(f'E:/GoogleSync/Masters/Dissertation/models/24-Jun-2021') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1170a1d3",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "27a9ab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30171a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/08/07 15:26:16 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0268s vs `on_train_batch_end` time: 0.0511s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0268s vs `on_train_batch_end` time: 0.0511s). Check your callbacks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192/192 - 8s - loss: 5.8024 - accuracy: 0.0366 - val_loss: 5.9437 - val_accuracy: 0.0204\n",
      "Epoch 2/300\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"custom-model-tests\"\n",
    "RUN_NAME = \"default_cnn_coloured\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_param(\"activation function\", act)\n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        #mlflow.keras.log_model(model, \"standardCNN\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db851db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow server --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9ac114",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function that returns loss value & metrics values\n",
    "model.evaluate(image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3ec96c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db31c612",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86168ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_accuracy(image_test, model):\n",
    "    #going through each of the test images\n",
    "    count = 0\n",
    "    for i in range(len(image_test)): #going through all test images\n",
    "        img = image_test[i] #load in the image\n",
    "        \n",
    "        img = img.reshape(-1, 100, 100, 3) #get it in the right shape for model.predict\n",
    "        \n",
    "        prediction = model.predict(img).flatten() #using model predict to get what it thinks is the answer\n",
    "        \n",
    "        # prediction.astype(np.int) #converts predictions to integers\n",
    "        \n",
    "        pred_index = np.argmax(prediction) # grabs the INDEX of the best prediction\n",
    "        \n",
    "        max_pred = max(prediction) # grabs the SCORE of best prediction\n",
    "        # print(prediction)\n",
    "        # print(pred_index)\n",
    "        \n",
    "        test_label_value = label_test[i][pred_index] # gets the actual test label at the same index\n",
    "        # print(test_label_value)\n",
    "        if test_label_value == 1:\n",
    "            #print(\"CORRECT PREDICTION\")\n",
    "            count += 1\n",
    "            \n",
    "#         # FIND THE INDEX WHERE THE VALUE = 1\n",
    "#         for j in range(len(predicted_label)):\n",
    "#             if predicted_label[j] == 1 : #find the point when it finds 1 \n",
    "#                 if label_test[i][j] == 1: #check if at this exact point is also 1  whic means correct prediction\n",
    "#                     count+=1 #increase number of correct count by 1\n",
    "#                     print(count)\n",
    "#                     break\n",
    "    print(f\"num correct - {count}\")\n",
    "    accuracy = (count/len(label_test))*100\n",
    "    per_symbol = '%'\n",
    "    print(f'accuracy is {accuracy} {per_symbol}')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446de09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad03f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_accuracy(image_test, model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks for best model\n",
    "runs = mlflow.search_runs(experiment_ids=experiment_id,\n",
    "                          order_by=['metrics.mae'], max_results=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6340490f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1d438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8233bce6",
   "metadata": {},
   "source": [
    "# Segmentation algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599480db",
   "metadata": {},
   "source": [
    "# MASK RCNN CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d290bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967e034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"E:\\Dissertation\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "# Import COCO config\n",
    "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
    "import coco\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "\n",
    "# Directory of images to run detection on\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"data\\img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104bfc39",
   "metadata": {},
   "source": [
    "## Extract Masks from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e724bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all the image paths stores in a list\n",
    "def get_img_paths(URL_dir):\n",
    "    img_paths = []\n",
    "    for path in os.listdir(URL_dir):\n",
    "        full_path = os.path.join(URL_dir, path)\n",
    "        if os.path.isfile(full_path):\n",
    "            img_paths.append(full_path)\n",
    "\n",
    "    return img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34762a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = get_img_paths(URL_dir)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b974129",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca326ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plots each object on a copy of the image and attempts to remove noise\n",
    "\n",
    "def plot_mask(image, masks):\n",
    "    all_masks = list()\n",
    "    masks = masks.astype(int)\n",
    "    # print(masks.shape)\n",
    "    \n",
    "    for i in range(masks.shape[2]):\n",
    "        temp = np.copy(image)\n",
    "        for j in range(temp.shape[2]):\n",
    "            temp[:,:,j] = temp[:,:,j] * masks[:,:,i]\n",
    "        all_masks.append(temp)\n",
    "#         plt.figure(figsize=(8,8))\n",
    "#         plt.imshow(temp)\n",
    "    # print(f\"Number of masks: {len(all_masks)}\")\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665be4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Overlay all masks onto a black canvas\n",
    "\n",
    "def combine_mask_objects(masks, shape):\n",
    "    canvas = np.zeros(shape, np.uint8)\n",
    "#     print(\"ORIGINAL CANVAS\")\n",
    "#     plt.figure(figsize=(8,8))\n",
    "#     plt.imshow(canvas)\n",
    "    for i, obj in enumerate(masks):\n",
    "        canvas = cv2.add(canvas, obj)\n",
    "#         plt.figure(figsize=(8,8))\n",
    "#         plt.imshow(canvas)\n",
    "    return canvas\n",
    "\n",
    "### Saves all masks for each image in a separate directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(img_id, img, folder):\n",
    "    # checks if the img_id folder exists, if not create one\n",
    "    img = PIL.Image.fromarray(img, 'RGB')\n",
    "    img.save(f'{folder}/{img_id}.png')\n",
    "    return True       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object in inference mode.\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "# Load weights trained on MS-COCO\n",
    "model.load_weights(COCO_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8978a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_masks(folder_name, img_paths):\n",
    "    # checks if the masks folder exists, if not creates one\n",
    "    print(os.getcwd())\n",
    "    if not os.path.isdir(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    for img_path in img_paths:\n",
    "        print(img_path)\n",
    "        img_id = Path(img_path).stem\n",
    "        print(img_id)\n",
    "        image = skimage.io.imread(img_path)\n",
    "        # check if image is in rgba format\n",
    "        if image.shape[2] > 3:\n",
    "            # convert to rgb\n",
    "            image = rgba2rgb(image)\n",
    "        # Run detection\n",
    "        results = model.detect([image], verbose=0)\n",
    "        # Visualize results\n",
    "        r = results[0]\n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], UNIQUE_LABELS, r['scores'])\n",
    "\n",
    "        masks = plot_mask(np.copy(image), r[\"masks\"])\n",
    "\n",
    "        if len(masks) > 0:\n",
    "            processed_image = combine_mask_objects(masks, masks[0].shape)\n",
    "            print(masks)\n",
    "            save_image(i, processed_image, folder_name)\n",
    "        else:\n",
    "            print(f\"{img_id} has no masks!\")\n",
    "            save_image(img_id, image, folder_name)\n",
    "    return \"COMPLETE!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee703fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join(\"E:\\Dissertation\")\n",
    "# os.chdir(path)\n",
    "# UNCOMMENT THIS LINE TO EXTRACT MASKS FROM DATATSET\n",
    "# BEWARE THIS PROCESS CAN TAKE HOURS TO FINISH!\n",
    "generate_masks(\"masks\", img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19d38c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MASK_DIR = os.path.join(MAIN_PATH, 'data\\train_masks')\n",
    "DEV_SEEN_MASK_DIR = os.path.join(MAIN_PATH, 'data\\dev_seen_masks')\n",
    "DEV_UNSEEN_MASK_DIR = os.path.join(MAIN_PATH, 'data\\dev_unseen_masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce7b56",
   "metadata": {},
   "source": [
    "# 3. Create feature vector of object masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d13d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def extract_id(name):\n",
    "    filename = Path(name).stem\n",
    "    # print(filename)\n",
    "    return filename\n",
    "\n",
    "def generate_data(df, directory):\n",
    "    # dataset con\n",
    "    ids = df[\"id\"].to_numpy()\n",
    "    labels = df[\"label\"].to_numpy()\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for root, _, files in os.walk(directory):\n",
    "        print(root)\n",
    "        count = 0\n",
    "        for file in tqdm(files):\n",
    "            # print(f\"IMAGE: {file}\")\n",
    "            # get full path of the mask\n",
    "            full_path = os.path.join(root, file)\n",
    "            # extract the image id from file\n",
    "            img_id = extract_id(file)\n",
    "            # get tensor of the mask\n",
    "            image = Image.open(full_path)\n",
    "            image = image.resize((224, 224))\n",
    "            image_vect = np.asarray(image)\n",
    "            # print(image_vect.shape)\n",
    "            # find the index for the image_id\n",
    "            idx = np.where(ids == int(img_id))[0][0]\n",
    "            # print(df.iloc[idx])\n",
    "\n",
    "            # get the label for that image_id\n",
    "            label = labels[idx]\n",
    "\n",
    "            # Append features and labels to data lists\n",
    "            X.append(image_vect)\n",
    "            y.append(label)\n",
    "        break\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_data(TRAIN_DF, TRAIN_MASK_DIR)\n",
    "\n",
    "# uncomment to generate training data for dev masks\n",
    "# X, y = generate_data(DEV_SEEN_DF, DEV_SEEN_MASK_DIR)\n",
    "# X, y = generate_data(DEV_UNSEEN_DF, DEV_UNSEEN_MASK_DIR)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c5b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335db64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedd41cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29721ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad216f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43996e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc119a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1364a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a01fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704716c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb928e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28e2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b90d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b52e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d4a541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335092d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd45548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649833bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a3ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b8c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
