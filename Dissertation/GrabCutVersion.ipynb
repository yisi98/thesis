{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a506cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###imports###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from scipy import io\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "import skimage.io\n",
    "import random\n",
    "import PIL\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10adc903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c83099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes all outputs be in float format rather than exponentials\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ef71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "IMG_SIZE = 100 #this parameter sets image dimensions as 50*50\n",
    "DATE = datetime.datetime.now().strftime('%d-%b-%Y')\n",
    "MODEL_PATH = f'models/{DATE}/'\n",
    "MODEL_NAME = 'FlowerClassifierTrial.model'.format(int(time.time()))\n",
    "TENSORBOARD = TensorBoard(log_dir=f'logs\\\\{MODEL_NAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b69b42b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Directory for PC\n",
    "labels_dir = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "URL = 'E:/Github/thesis/flowerDataset/grabcut'\n",
    "dataSplit = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "loaded_images_dir = 'E:/Dissertation/GrabCutloaded_images.npy'\n",
    "image_train_dir = 'E:/Dissertation/GCimage_train.npy'\n",
    "label_train_dir = 'E:/Dissertation/label_train.npy'\n",
    "image_test_dir = 'E:/Dissertation/GCimage_test.npy'\n",
    "label_test_dir = 'E:/Dissertation/label_test.npy'\n",
    "image_val_dir = 'E:/Dissertation/GCimage_val.npy'\n",
    "label_val_dir = 'E:/Dissertation/GClabel_val.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92746dda",
   "metadata": {},
   "source": [
    "# labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec809cd",
   "metadata": {},
   "source": [
    "# Load in Images for GrabCut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bb7541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets all the image paths stores in a list\n",
    "def get_img_paths(URL):\n",
    "    img_paths = []\n",
    "    for path in os.listdir(URL_dir):\n",
    "        full_path = os.path.join(URL_dir, path)\n",
    "        if os.path.isfile(full_path):\n",
    "            img_paths.append(full_path)\n",
    "\n",
    "    return img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0e15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_paths = get_img_paths(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f5cec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_cut(img_paths): #grab cut algorithm used to created segmented flower images WARNING: very time consuming\n",
    "    for i in range(len(img_paths)):\n",
    "        img = cv2.imread(img_paths[i])\n",
    "        name = Path(img_paths[i]).stem\n",
    "        loc = 'E:/Github/thesis/flowerDataset/grabcut/'\n",
    "        mask = np.zeros(img.shape[:2], np.uint8)\n",
    "\n",
    "        bgdModel = np.zeros((1,65), np.float64)\n",
    "        fgdModel = np.zeros((1,65), np.float64)\n",
    "        rect = (10,5,650,650)\n",
    "\n",
    "        cv2.grabCut(img,mask,rect,bgdModel,fgdModel,5,cv2.GC_INIT_WITH_RECT)\n",
    "        mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "        img = img*mask2[:,:,np.newaxis]\n",
    "\n",
    "        cv2.imwrite(loc+ name + '.jpg', img)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6489e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab_cut(img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a54feb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got loading files code from link below\n",
    "#https://stackoverflow.com/questions/30230592/loading-all-images-using-imread-from-a-given-folder\n",
    "#Function gets all the images loaded up\n",
    "def load_images_from_folder(URL):\n",
    "    images = []\n",
    "    for filename in os.listdir(URL):\n",
    "        img = cv2.imread(os.path.join(URL,filename))\n",
    "        img = cv2.resize(cv2.imread(os.path.join(URL,filename), cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "        img = np.reshape(img,[IMG_SIZE,IMG_SIZE,3])\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    np.save(loaded_images_dir, images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab3a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to run this cell if image already loaded.\n",
    "#load_images_from_folder(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4437eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.load(loaded_images_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a9770c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(label_val_dir, allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f51376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "labels = np.array(mlb.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5ea9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citation code from https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "#splitting the image dataset into the ratio for training, validation adn testing data\n",
    "def split_data(image_data, labels):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\n",
    "    image_train, image_test, label_train, label_test = train_test_split(image_data, labels, test_size=1 - train_ratio)\n",
    "\n",
    "    #the test from previous line which is 25% of dataset is passed into the line below to be \n",
    "    #further split into 15% for validation and 10% for testing\n",
    "\n",
    "    image_val, image_test, label_val, label_test = train_test_split(image_test, label_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
    "\n",
    "    print('image_train',image_train.shape)\n",
    "    print('label_train',label_train.shape)\n",
    "    print('image_test',image_test.shape)\n",
    "    print('label_test',label_test.shape)\n",
    "    print('image_val', image_val.shape)\n",
    "    print('label_val', label_val.shape)\n",
    "\n",
    "    np.save(image_train_dir, image_train)\n",
    "    np.save(label_train_dir, label_train)\n",
    "    np.save(image_test_dir, image_test)\n",
    "    np.save(label_test_dir, label_test)\n",
    "    np.save(image_val_dir, image_val)\n",
    "    np.save(label_val_dir, label_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d6a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split_data(image_data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd55423",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e5e7d52cd0cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_train_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimage_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_test_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlabel_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_train_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlabel_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_test_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_val_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "image_train = np.load(image_train_dir, allow_pickle=True)\n",
    "image_test = np.load(image_test_dir, allow_pickle=True)\n",
    "label_train = np.load(label_train_dir, allow_pickle=True)\n",
    "label_test = np.load(label_test_dir, allow_pickle=True)\n",
    "image_val = np.load(image_val_dir, allow_pickle=True)\n",
    "label_val = np.load(label_val_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc7105",
   "metadata": {},
   "source": [
    "# Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92e7e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts the images to the range of 0 - 1.0 \n",
    "image_train = image_train/255.0\n",
    "image_test = image_test/255.0\n",
    "image_val = image_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "254cae23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of classes/ flowers\n",
    "NUM_CLASSES = len(labels[0])\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10eddb3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aaef75baabce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimage_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'image_train' is not defined"
     ]
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7faec0e",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "342a6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of model\n",
    "name_model = 'GC-{}'.format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "371e622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir = 'logs/{}'.format(name_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "967217d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 6\n",
    "epochs = 30\n",
    "verbose= 2\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b20dbf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(act, verbose):\n",
    "    model = Sequential()\n",
    "    input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "    \n",
    "    # INPUT LAYER\n",
    "    model.add(Conv2D(32, (3, 3), activation=act, input_shape=input_shape))\n",
    "    ## model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # HIDDEN LAYER 1\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation(act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # HIDDEN LAYER 2\n",
    "    model.add(Conv2D(64, (3, 3), activation=act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 3\n",
    "    model.add(Conv2D(128, (3, 3), activation=act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "    \n",
    "    # HIDDEN LAYER 4\n",
    "    model.add(Conv2D(256, (3, 3), activation=act))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.25))\n",
    "\n",
    "    # Fully Connected\n",
    "    model.add(Flatten()) # converts the 3D feature maps to 1D feature vectors\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.25)) # reduces overfitting\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    model.add(Dense(NUM_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f91bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_model(activation, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c525e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 98, 98, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 96, 96, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 96, 96, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 23, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 10, 10, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1048832   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 102)               26214     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 102)               0         \n",
      "=================================================================\n",
      "Total params: 1,474,630\n",
      "Trainable params: 1,473,670\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c1017",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "131be153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89061b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\yisi9\\AppData\\Local\\Temp\\tmpyrq6hno5\\model\\data\\model\\assets\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\yisi9\\AppData\\Local\\Temp\\tmp9v5ujrxc\\model\\data\\model\\assets\n"
     ]
    }
   ],
   "source": [
    "mlflow.keras.autolog()\n",
    "\n",
    "timed = time() - start_time\n",
    "\n",
    "with mlflow.start_run():\n",
    "        \n",
    "        model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "        # Fit our model\n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val))\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = 0)\n",
    "        \n",
    "        mlflow.log_param(\"activation function\", activation)\n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        \n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        mlflow.keras.log_model(model, \"GB1stmodel\")\n",
    "        print('time took: ', timed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "659cfb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd74e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://localhost:5000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9d2032d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.948765277862549, 0.009756097570061684]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(image_test, label_test, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868af19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
