{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "737f5f14",
   "metadata": {},
   "source": [
    "# Dissertation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3073d9",
   "metadata": {},
   "source": [
    "## Research into the techniques and methods to achieve state of the art accuracy in flower species identification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0195ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d6984ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###imports###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "from scipy import io\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.keras.optimizer_v2.adam import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import datasets, layers, models, losses, Model\n",
    "import keras\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import logging\n",
    "#import skimage.io\n",
    "import random\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa11d15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a43f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fed53ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name(): \n",
    "\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ecf1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes all outputs be in float format rather than exponentials\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c860a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Parameters\n",
    "IMG_SIZE = 224 #this parameter sets image dimensions as 50*50\n",
    "DATE = datetime.datetime.now().strftime('%d-%b-%Y')\n",
    "MODEL_PATH = f'models/{DATE}/'\n",
    "MODEL_NAME = 'TransferResNet152V2.model'.format(int(time.time()))\n",
    "TENSORBOARD = TensorBoard(log_dir=f'logs\\\\{MODEL_NAME}')\n",
    "log_dir=f'logs\\\\{MODEL_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c687c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825fca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbdd12",
   "metadata": {},
   "source": [
    "# Load Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee55642",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_dir = 'E:/Github/thesis/flowerDataset/Flower'\n",
    "labels_dir = 'E:/Github/thesis/flowerDataset/imagelabels.mat'\n",
    "dataSplit_dir = 'E:/Github/thesis/flowerDataset/setid.mat'\n",
    "loaded_images_dir = 'E:/Dissertation/loaded_images.npy'\n",
    "image_train_dir ='E:/Dissertation/image_train.npy'\n",
    "label_train_dir = 'E:/Dissertation/label_train.npy'\n",
    "image_test_dir = 'E:/Dissertation/image_test.npy'\n",
    "label_test_dir = 'E:/Dissertation/label_test.npy'\n",
    "image_val_dir = 'E:/Dissertation/image_val.npy'\n",
    "label_val_dir = 'E:/Dissertation/label_val.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606a621e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_labels = io.loadmat(labels_dir)\n",
    "data_labels = data_labels.items()\n",
    "data_labels = list(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8764eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "954c09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[1][3][0] #loads the labels that is stored in dataframe format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f42638c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8189"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16fc6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got loading files code from link below\n",
    "#https://stackoverflow.com/questions/30230592/loading-all-images-using-imread-from-a-given-folder\n",
    "#Function gets all the images loaded up\n",
    "def load_images_from_folder(URL):\n",
    "    images = []\n",
    "    for filename in os.listdir(URL):\n",
    "        #img = cv2.imread(os.path.join(URL,filename))\n",
    "        img = cv2.resize(cv2.imread(os.path.join(URL,filename), cv2.COLOR_BGR2RGB), (IMG_SIZE, IMG_SIZE))\n",
    "        img = np.reshape(img,[IMG_SIZE,IMG_SIZE,3])\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    np.save(loaded_images_dir, images)\n",
    "#################################    return images#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ce45dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No need to run this cell if image already loaded.\n",
    "load_images_from_folder(URL_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfa034b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = np.load(loaded_images_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57859247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8189, 224, 224, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09dc7686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomised_dataset(image_data, labels):\n",
    "    np.random.seed(8)\n",
    "    indices = np.arange(image_data.shape[0])\n",
    "    print(indices)\n",
    "    np.random.shuffle(indices)\n",
    "    image_data = image_data[indices]\n",
    "    labels = labels[indices]\n",
    "    return image_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "306569b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 8186 8187 8188]\n"
     ]
    }
   ],
   "source": [
    "image_data, labels = randomised_dataset(image_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21d46713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Label Encoder\n",
    "\n",
    "mlb = LabelBinarizer()\n",
    "converted_labels = np.array(mlb.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d3a8adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dbee6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#citation code from https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn\n",
    "#splitting the image dataset into the ratio for training, validation and testing data\n",
    "def split_data(image_data, converted_labels):\n",
    "    \n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\n",
    "    image_train, image_test, label_train, label_test = train_test_split(image_data, converted_labels, test_size=1 - train_ratio, random_state=42)\n",
    "\n",
    "    #the test from previous line which is 25% of dataset is passed into the line below to be \n",
    "    #further split into 15% for validation and 10% for testing\n",
    "\n",
    "    image_val, image_test, label_val, label_test = train_test_split(image_test, label_test, test_size=test_ratio/(test_ratio + validation_ratio),random_state=42) \n",
    "\n",
    "    print('image_train',image_train.shape)\n",
    "    print('label_train',label_train.shape)\n",
    "    print('image_test',image_test.shape)\n",
    "    print('label_test',label_test.shape)\n",
    "    print('image_val', image_val.shape)\n",
    "    print('label_val', label_val.shape)\n",
    "\n",
    "    np.save(image_train_dir, image_train)\n",
    "    np.save(label_train_dir, label_train)\n",
    "    np.save(image_test_dir, image_test)\n",
    "    np.save(label_test_dir, label_test)\n",
    "    np.save(image_val_dir, image_val)\n",
    "    np.save(label_val_dir, label_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "714c1c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train (6141, 224, 224, 3)\n",
      "label_train (6141, 102)\n",
      "image_test (820, 224, 224, 3)\n",
      "label_test (820, 102)\n",
      "image_val (1228, 224, 224, 3)\n",
      "label_val (1228, 102)\n"
     ]
    }
   ],
   "source": [
    "split_data(image_data,converted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b07b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.load(image_train_dir, allow_pickle=True)\n",
    "image_test = np.load(image_test_dir, allow_pickle=True)\n",
    "label_train = np.load(label_train_dir, allow_pickle=True)\n",
    "label_test = np.load(label_test_dir, allow_pickle=True)\n",
    "image_val = np.load(image_val_dir, allow_pickle=True)\n",
    "label_val = np.load(label_val_dir, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74fa7964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 224, 224, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a1187",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff6920dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #converts the images to the range of 0 - 1.0 \n",
    "image_train = image_train/255.0\n",
    "image_test = image_test/255.0\n",
    "image_val = image_val/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1bb389a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 224, 224, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e534027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6141, 102)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "312de917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(820, 102)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4eeeb8c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac508a3",
   "metadata": {},
   "source": [
    "## Find unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27bb0e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6141"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55d0ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes/ flowers\n",
    "NUM_CLASSES = 102"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d89c64",
   "metadata": {},
   "source": [
    "# Transfer Learning --ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5aecf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications  import ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "600f7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet152V2(\n",
    "    input_shape=(224,224,3),\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False, # removes last layer of the resnet model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b7d5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet.layers: #don't train existing weights\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c48fd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(resnet.output)\n",
    "x = layers.Dense(1000, activation='relu')(x)\n",
    "prediction = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b0912eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Total params: 158,786,750\n",
      "Trainable params: 100,455,102\n",
      "Non-trainable params: 58,331,648\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "67a12b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0da386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2a00779",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "verbose= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b83a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/08/22 11:04:00 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of tensorflow. If you encounter errors during autologging, try upgrading / downgrading tensorflow to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... 'input_1_ib-0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... 'input_1_ib-0'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "96/96 - 36s - loss: 5.6126 - accuracy: 0.4809 - val_loss: 1.5935 - val_accuracy: 0.6523\n",
      "Epoch 2/100\n",
      "96/96 - 17s - loss: 0.1823 - accuracy: 0.9479 - val_loss: 1.2610 - val_accuracy: 0.7353\n",
      "Epoch 3/100\n",
      "96/96 - 17s - loss: 0.0469 - accuracy: 0.9907 - val_loss: 1.3310 - val_accuracy: 0.7419\n",
      "Epoch 4/100\n",
      "96/96 - 17s - loss: 0.0312 - accuracy: 0.9920 - val_loss: 1.4959 - val_accuracy: 0.7174\n",
      "Epoch 5/100\n",
      "96/96 - 17s - loss: 0.0118 - accuracy: 0.9982 - val_loss: 1.3035 - val_accuracy: 0.7508\n",
      "Epoch 6/100\n",
      "96/96 - 17s - loss: 0.0040 - accuracy: 0.9998 - val_loss: 1.2697 - val_accuracy: 0.7557\n",
      "Epoch 7/100\n",
      "96/96 - 17s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 1.2367 - val_accuracy: 0.7598\n",
      "Epoch 8/100\n",
      "96/96 - 17s - loss: 6.5748e-04 - accuracy: 0.9998 - val_loss: 1.2323 - val_accuracy: 0.7614\n",
      "Epoch 9/100\n",
      "96/96 - 17s - loss: 7.0605e-04 - accuracy: 0.9997 - val_loss: 1.2347 - val_accuracy: 0.7598\n",
      "Epoch 10/100\n",
      "96/96 - 17s - loss: 3.1026e-04 - accuracy: 1.0000 - val_loss: 1.2341 - val_accuracy: 0.7622\n",
      "Epoch 11/100\n",
      "96/96 - 17s - loss: 5.0067e-04 - accuracy: 0.9998 - val_loss: 1.2345 - val_accuracy: 0.7655\n",
      "Epoch 12/100\n",
      "96/96 - 17s - loss: 3.9795e-04 - accuracy: 0.9998 - val_loss: 1.2414 - val_accuracy: 0.7655\n",
      "Epoch 13/100\n",
      "96/96 - 17s - loss: 5.9061e-04 - accuracy: 0.9998 - val_loss: 1.2401 - val_accuracy: 0.7663\n",
      "Epoch 14/100\n",
      "96/96 - 17s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 1.2717 - val_accuracy: 0.7638\n",
      "Epoch 15/100\n",
      "96/96 - 17s - loss: 0.0018 - accuracy: 0.9998 - val_loss: 1.2457 - val_accuracy: 0.7655\n",
      "Epoch 16/100\n",
      "96/96 - 17s - loss: 6.2630e-04 - accuracy: 0.9997 - val_loss: 1.2383 - val_accuracy: 0.7695\n",
      "Epoch 17/100\n",
      "96/96 - 17s - loss: 3.3789e-04 - accuracy: 0.9998 - val_loss: 1.2392 - val_accuracy: 0.7687\n",
      "Epoch 18/100\n",
      "96/96 - 17s - loss: 3.3931e-04 - accuracy: 0.9997 - val_loss: 1.2404 - val_accuracy: 0.7687\n",
      "Epoch 19/100\n",
      "96/96 - 18s - loss: 3.7807e-04 - accuracy: 0.9997 - val_loss: 1.2373 - val_accuracy: 0.7679\n",
      "Epoch 20/100\n",
      "96/96 - 20s - loss: 4.1326e-04 - accuracy: 0.9998 - val_loss: 1.2350 - val_accuracy: 0.7712\n",
      "Epoch 21/100\n",
      "96/96 - 19s - loss: 5.2073e-04 - accuracy: 0.9998 - val_loss: 1.2554 - val_accuracy: 0.7663\n",
      "Epoch 22/100\n",
      "96/96 - 19s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 1.2356 - val_accuracy: 0.7720\n",
      "Epoch 23/100\n",
      "96/96 - 20s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 1.2558 - val_accuracy: 0.7679\n",
      "Epoch 24/100\n",
      "96/96 - 20s - loss: 9.4485e-04 - accuracy: 0.9998 - val_loss: 1.2423 - val_accuracy: 0.7712\n",
      "Epoch 25/100\n",
      "96/96 - 20s - loss: 8.1431e-04 - accuracy: 1.0000 - val_loss: 1.3076 - val_accuracy: 0.7687\n",
      "Epoch 26/100\n",
      "96/96 - 19s - loss: 0.0025 - accuracy: 0.9997 - val_loss: 1.2737 - val_accuracy: 0.7728\n",
      "Epoch 27/100\n",
      "96/96 - 18s - loss: 0.0024 - accuracy: 0.9997 - val_loss: 1.2353 - val_accuracy: 0.7752\n",
      "Epoch 28/100\n",
      "96/96 - 18s - loss: 4.5274e-04 - accuracy: 0.9998 - val_loss: 1.2387 - val_accuracy: 0.7728\n",
      "Epoch 29/100\n",
      "96/96 - 19s - loss: 6.2336e-04 - accuracy: 0.9997 - val_loss: 1.2386 - val_accuracy: 0.7744\n",
      "Epoch 30/100\n",
      "96/96 - 18s - loss: 3.5681e-04 - accuracy: 0.9997 - val_loss: 1.2373 - val_accuracy: 0.7744\n",
      "Epoch 31/100\n",
      "96/96 - 18s - loss: 2.8678e-04 - accuracy: 0.9998 - val_loss: 1.2378 - val_accuracy: 0.7712\n",
      "Epoch 32/100\n",
      "96/96 - 18s - loss: 3.9261e-04 - accuracy: 0.9997 - val_loss: 1.2378 - val_accuracy: 0.7728\n",
      "Epoch 33/100\n",
      "96/96 - 18s - loss: 3.1919e-04 - accuracy: 0.9998 - val_loss: 1.2390 - val_accuracy: 0.7720\n",
      "Epoch 34/100\n",
      "96/96 - 18s - loss: 3.6824e-04 - accuracy: 0.9998 - val_loss: 1.2388 - val_accuracy: 0.7744\n",
      "Epoch 35/100\n",
      "96/96 - 17s - loss: 4.1062e-04 - accuracy: 0.9998 - val_loss: 1.2396 - val_accuracy: 0.7712\n",
      "Epoch 36/100\n",
      "96/96 - 17s - loss: 5.0231e-04 - accuracy: 0.9998 - val_loss: 1.2477 - val_accuracy: 0.7720\n",
      "Epoch 37/100\n",
      "96/96 - 17s - loss: 6.4410e-04 - accuracy: 0.9998 - val_loss: 1.2423 - val_accuracy: 0.7769\n",
      "Epoch 38/100\n",
      "96/96 - 18s - loss: 6.6630e-04 - accuracy: 0.9998 - val_loss: 1.2417 - val_accuracy: 0.7744\n",
      "Epoch 39/100\n",
      "96/96 - 17s - loss: 4.2738e-04 - accuracy: 0.9998 - val_loss: 1.2480 - val_accuracy: 0.7752\n",
      "Epoch 40/100\n",
      "96/96 - 17s - loss: 3.5625e-04 - accuracy: 0.9998 - val_loss: 1.3295 - val_accuracy: 0.7720\n",
      "Epoch 41/100\n",
      "96/96 - 17s - loss: 7.2133e-04 - accuracy: 0.9998 - val_loss: 1.2822 - val_accuracy: 0.7728\n",
      "Epoch 42/100\n",
      "96/96 - 17s - loss: 0.0030 - accuracy: 0.9997 - val_loss: 1.2730 - val_accuracy: 0.7695\n",
      "Epoch 43/100\n",
      "96/96 - 17s - loss: 0.0015 - accuracy: 0.9998 - val_loss: 1.2802 - val_accuracy: 0.7704\n",
      "Epoch 44/100\n",
      "96/96 - 17s - loss: 0.0024 - accuracy: 0.9997 - val_loss: 1.2925 - val_accuracy: 0.7695\n",
      "Epoch 45/100\n",
      "96/96 - 17s - loss: 0.0015 - accuracy: 0.9998 - val_loss: 1.2740 - val_accuracy: 0.7695\n",
      "Epoch 46/100\n",
      "96/96 - 17s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 1.2701 - val_accuracy: 0.7695\n",
      "Epoch 47/100\n",
      "96/96 - 17s - loss: 0.0021 - accuracy: 0.9995 - val_loss: 2.5066 - val_accuracy: 0.6637\n",
      "Epoch 48/100\n",
      "96/96 - 17s - loss: 4.3313 - accuracy: 0.4656 - val_loss: 3.0121 - val_accuracy: 0.4389\n",
      "Epoch 49/100\n",
      "96/96 - 17s - loss: 1.5363 - accuracy: 0.6924 - val_loss: 2.9904 - val_accuracy: 0.5643\n",
      "Epoch 50/100\n",
      "96/96 - 17s - loss: 0.6936 - accuracy: 0.8471 - val_loss: 2.5278 - val_accuracy: 0.6352\n",
      "Epoch 51/100\n",
      "96/96 - 17s - loss: 0.2819 - accuracy: 0.9324 - val_loss: 2.4857 - val_accuracy: 0.6702\n",
      "Epoch 52/100\n",
      "96/96 - 17s - loss: 0.1143 - accuracy: 0.9720 - val_loss: 2.7875 - val_accuracy: 0.6751\n",
      "Epoch 53/100\n",
      "96/96 - 17s - loss: 0.0505 - accuracy: 0.9834 - val_loss: 2.6167 - val_accuracy: 0.6914\n",
      "Epoch 54/100\n",
      "96/96 - 17s - loss: 0.0250 - accuracy: 0.9938 - val_loss: 2.7444 - val_accuracy: 0.6979\n",
      "Epoch 55/100\n",
      "96/96 - 17s - loss: 0.0242 - accuracy: 0.9932 - val_loss: 2.7187 - val_accuracy: 0.6832\n",
      "Epoch 56/100\n",
      "96/96 - 17s - loss: 0.0153 - accuracy: 0.9977 - val_loss: 2.5766 - val_accuracy: 0.7109\n",
      "Epoch 57/100\n",
      "96/96 - 17s - loss: 0.0053 - accuracy: 0.9987 - val_loss: 2.6205 - val_accuracy: 0.7060\n",
      "Epoch 58/100\n",
      "96/96 - 17s - loss: 0.0054 - accuracy: 0.9992 - val_loss: 2.6315 - val_accuracy: 0.7117\n",
      "Epoch 59/100\n",
      "96/96 - 17s - loss: 0.0044 - accuracy: 0.9992 - val_loss: 2.6366 - val_accuracy: 0.7117\n",
      "Epoch 60/100\n",
      "96/96 - 17s - loss: 0.0022 - accuracy: 0.9993 - val_loss: 2.6666 - val_accuracy: 0.7117\n",
      "Epoch 61/100\n",
      "96/96 - 17s - loss: 0.0020 - accuracy: 0.9995 - val_loss: 2.6843 - val_accuracy: 0.7134\n",
      "Epoch 62/100\n",
      "96/96 - 17s - loss: 0.0018 - accuracy: 0.9995 - val_loss: 2.7056 - val_accuracy: 0.7158\n",
      "Epoch 63/100\n",
      "96/96 - 17s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 2.7203 - val_accuracy: 0.7093\n",
      "Epoch 64/100\n",
      "96/96 - 17s - loss: 0.0025 - accuracy: 0.9997 - val_loss: 2.7189 - val_accuracy: 0.7117\n",
      "Epoch 65/100\n",
      "96/96 - 17s - loss: 0.0023 - accuracy: 0.9997 - val_loss: 2.7102 - val_accuracy: 0.7125\n",
      "Epoch 66/100\n",
      "96/96 - 17s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 2.7190 - val_accuracy: 0.7125\n",
      "Epoch 67/100\n",
      "96/96 - 17s - loss: 0.0014 - accuracy: 0.9995 - val_loss: 2.7217 - val_accuracy: 0.7142\n",
      "Epoch 68/100\n",
      "96/96 - 17s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 2.7351 - val_accuracy: 0.7166\n",
      "Epoch 69/100\n",
      "96/96 - 17s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 2.7459 - val_accuracy: 0.7142\n",
      "Epoch 70/100\n",
      "96/96 - 17s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 2.7491 - val_accuracy: 0.7134\n",
      "Epoch 71/100\n",
      "96/96 - 17s - loss: 0.0022 - accuracy: 0.9997 - val_loss: 2.7474 - val_accuracy: 0.7125\n",
      "Epoch 72/100\n",
      "96/96 - 17s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 2.7541 - val_accuracy: 0.7125\n",
      "Epoch 73/100\n",
      "96/96 - 17s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 2.7562 - val_accuracy: 0.7109\n",
      "Epoch 74/100\n",
      "96/96 - 17s - loss: 0.0011 - accuracy: 0.9997 - val_loss: 2.7613 - val_accuracy: 0.7117\n",
      "Epoch 75/100\n",
      "96/96 - 17s - loss: 0.0012 - accuracy: 0.9995 - val_loss: 2.7658 - val_accuracy: 0.7134\n",
      "Epoch 76/100\n",
      "96/96 - 17s - loss: 0.0010 - accuracy: 0.9997 - val_loss: 2.7733 - val_accuracy: 0.7142\n",
      "Epoch 77/100\n",
      "96/96 - 17s - loss: 0.0012 - accuracy: 0.9997 - val_loss: 2.7761 - val_accuracy: 0.7142\n",
      "Epoch 78/100\n",
      "96/96 - 17s - loss: 0.0013 - accuracy: 0.9997 - val_loss: 2.7793 - val_accuracy: 0.7101\n",
      "Epoch 79/100\n",
      "96/96 - 17s - loss: 0.0015 - accuracy: 0.9995 - val_loss: 2.7794 - val_accuracy: 0.7134\n",
      "Epoch 80/100\n",
      "96/96 - 17s - loss: 0.0016 - accuracy: 0.9997 - val_loss: 2.8035 - val_accuracy: 0.7158\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96/96 - 17s - loss: 0.0038 - accuracy: 0.9992 - val_loss: 2.8483 - val_accuracy: 0.6979\n",
      "Epoch 82/100\n",
      "96/96 - 17s - loss: 0.3995 - accuracy: 0.9336 - val_loss: 4.7204 - val_accuracy: 0.6116\n",
      "Epoch 83/100\n",
      "96/96 - 17s - loss: 1.0538 - accuracy: 0.8792 - val_loss: 5.1493 - val_accuracy: 0.6132\n",
      "Epoch 84/100\n",
      "96/96 - 17s - loss: 0.4331 - accuracy: 0.9370 - val_loss: 4.8566 - val_accuracy: 0.6580\n",
      "Epoch 85/100\n",
      "96/96 - 17s - loss: 0.2731 - accuracy: 0.9665 - val_loss: 4.8847 - val_accuracy: 0.6775\n",
      "Epoch 86/100\n",
      "96/96 - 17s - loss: 0.0874 - accuracy: 0.9844 - val_loss: 4.4004 - val_accuracy: 0.7028\n",
      "Epoch 87/100\n",
      "96/96 - 17s - loss: 0.0490 - accuracy: 0.9902 - val_loss: 4.4966 - val_accuracy: 0.7109\n",
      "Epoch 88/100\n",
      "96/96 - 17s - loss: 0.0186 - accuracy: 0.9969 - val_loss: 4.3135 - val_accuracy: 0.7288\n",
      "Epoch 89/100\n",
      "96/96 - 17s - loss: 0.0109 - accuracy: 0.9971 - val_loss: 4.4104 - val_accuracy: 0.7182\n",
      "Epoch 90/100\n",
      "96/96 - 17s - loss: 0.0034 - accuracy: 0.9989 - val_loss: 4.5540 - val_accuracy: 0.7256\n",
      "Epoch 91/100\n",
      "96/96 - 17s - loss: 0.0027 - accuracy: 0.9992 - val_loss: 4.3697 - val_accuracy: 0.7370\n",
      "Epoch 92/100\n",
      "96/96 - 17s - loss: 0.0019 - accuracy: 0.9993 - val_loss: 4.2309 - val_accuracy: 0.7419\n",
      "Epoch 93/100\n",
      "96/96 - 18s - loss: 0.0152 - accuracy: 0.9984 - val_loss: 4.4143 - val_accuracy: 0.7223\n",
      "Epoch 94/100\n",
      "96/96 - 18s - loss: 0.0067 - accuracy: 0.9985 - val_loss: 4.4726 - val_accuracy: 0.7142\n",
      "Epoch 95/100\n",
      "96/96 - 17s - loss: 0.0033 - accuracy: 0.9993 - val_loss: 4.2672 - val_accuracy: 0.7199\n",
      "Epoch 96/100\n",
      "96/96 - 18s - loss: 0.0018 - accuracy: 0.9993 - val_loss: 4.1537 - val_accuracy: 0.7166\n",
      "Epoch 97/100\n",
      "96/96 - 18s - loss: 6.9014e-04 - accuracy: 0.9998 - val_loss: 4.0838 - val_accuracy: 0.7256\n",
      "Epoch 98/100\n",
      "96/96 - 18s - loss: 2.4890e-05 - accuracy: 1.0000 - val_loss: 4.0785 - val_accuracy: 0.7256\n",
      "Epoch 99/100\n",
      "96/96 - 18s - loss: 2.0604e-05 - accuracy: 1.0000 - val_loss: 4.0764 - val_accuracy: 0.7256\n",
      "Epoch 100/100\n",
      "96/96 - 17s - loss: 1.8432e-05 - accuracy: 1.0000 - val_loss: 4.0739 - val_accuracy: 0.7256\n",
      "13/13 - 2s - loss: 4.2359 - accuracy: 0.7207\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_NAME = \"transfer-learning-tests\"\n",
    "RUN_NAME = \"resnet152v2_Dense2\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f7adae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "winsound.Beep(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c01a95c",
   "metadata": {},
   "source": [
    "# ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a373f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications  import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81404988",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet50(\n",
    "    input_shape=(224,224,3),\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False, # removes last layer of the resnet model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet.layers: #don't train existing weights\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c470db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(resnet.output)\n",
    "\n",
    "prediction = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07726a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f80435",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "verbose= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba99e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"transfer-learning-tests\"\n",
    "RUN_NAME = \"resnet_50\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6868bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "winsound.Beep(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294f202",
   "metadata": {},
   "source": [
    "# ResNet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852435b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications  import ResNet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e638775",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet101(\n",
    "    input_shape=(224,224,3),\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False, # removes last layer of the resnet model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38083ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in resnet.layers: #don't train existing weights\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(resnet.output)\n",
    "\n",
    "prediction = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=resnet.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05e7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "verbose= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"transfer-learning-tests\"\n",
    "RUN_NAME = \"resnet_101\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde61910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "winsound.Beep(1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761da27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83a754a",
   "metadata": {},
   "source": [
    "# Experiment InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a01fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.InceptionV3(input_shape=(224,224,3),\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abba910",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers: #don't train existing weights\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(model.output)\n",
    "\n",
    "prediction = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704716c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb928e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_inputs = model.layers[0].input\n",
    "# base_outputs = model.layers[-2].output\n",
    "# final_outputs = layers.Dense(102)(base_outputs)\n",
    "# model = keras.Model(inputs=base_inputs, outputs=final_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28e2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"Adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b90d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f4b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"transfer-learning-tests\"\n",
    "RUN_NAME = \"inception_v3\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06d212",
   "metadata": {},
   "source": [
    "# VGG 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e59283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28280be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7a67da",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(50, activation='relu')\n",
    "dense_layer_2 = layers.Dense(20, activation='relu')\n",
    "prediction_layer = layers.Dense(NUM_CLASSES, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d689c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "verbose= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d22b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef950e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"transfer-learning-tests\"\n",
    "RUN_NAME = \"vgg_16\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39bf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "676014d1",
   "metadata": {},
   "source": [
    "# VGG 19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abdac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG19(weights=\"imagenet\", include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d1165",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(100, activation='relu')\n",
    "dense_layer_2 = layers.Dense(100, activation='relu')\n",
    "prediction_layer = layers.Dense(NUM_CLASSES, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402547b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "verbose= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21342d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1658e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"transfer-learning-tests\"\n",
    "RUN_NAME = \"vgg_19\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4307a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "verbose= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f0193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16c98cf6",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaca681",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.xception.Xception(\n",
    "    include_top=False, weights='imagenet', \n",
    "    input_shape=(224,224,3),\n",
    "    classifier_activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de216f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(100, activation='relu')\n",
    "dense_layer_2 = layers.Dense(100, activation='relu')\n",
    "prediction_layer = layers.Dense(NUM_CLASSES, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22ced17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c924b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"transfer-learning-tests\"\n",
    "RUN_NAME = \"xception\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.set_tracking_uri('file:///E:/GoogleSync/Masters/Dissertation/MLflow/mlruns')\n",
    "\n",
    "with mlflow.start_run(run_name=RUN_NAME) as run:\n",
    "        \n",
    "        mlflow.tensorflow.autolog()\n",
    "        \n",
    "        history = model.fit(image_train, label_train, batch_size=batch_size, epochs=epochs, verbose=verbose, validation_data=(image_val, label_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "        score = model.evaluate(image_test, label_test, batch_size=batch_size, verbose = verbose)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        mlflow.log_metric(\"test loss\", score[0])\n",
    "        mlflow.log_metric(\"test accuracy\", score[1])\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8933c587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
